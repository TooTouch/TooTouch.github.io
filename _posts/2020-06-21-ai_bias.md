---
title: "Bias in Data-driven AI Systems -
An Introductory Survey - Introduction"
categories: 
    - Paper Review
toc: true
---

# 시작말

오늘도 또다시 읽어보고 싶은 논문들을 줍줍하다가 그간 관심있게 생각했던 인공지능 속 Bais에 대한 논문을 알게되서 글도 읽고 정리도 해볼겸 글을 작성하게 되었습니다. 

논문 제목은 "Bias in Data-driven AI Systems -
An Introductory Survey" 이고 올해인 2020년 초에 나온 논문입니다. 저자들은 모두 유럽쪽 인사들이였고 내용에서 또한 인공지능에 대한 규제에 대해서 유럽에 상황들과 엮어서 소개하고 있습니다. 그러나 인공지능에 대한 규제는 단순히 유럽에서만은 문제는 아니기에 꼭 제한적인 내용은 아니라 생각됩니다. 

아래 이미지에서 볼 수 있듯이 논문의 내용은 크게 세 가지로 구성되어 있습니다. 따라서 포스팅도 세 번에 나눠서 작성하려 합니다.

1. **Understanding bias** - bais가 우리 사회속에 어떻게 생기고, 어떻게 사회-기술적 체제(socio-technical systems)에 들어오는지 설명하고 공정함(fairness)에 대한 정의를 다룹니다.
2. **Mitiating bias** - AI가 의사결정을 하는 단계마다 (pre-processing, in-processing 그리고 post-processing 추가로 데이터, 학습 알고리즘 그리고 모델 결과) bias를 다루는 방법을 다룹니다.
3. **Accounting for bias** - AI의 의사결정을 사람이 이해할 수 있는 방법으로 설명하여 사전에 bais 대처할 수 있는 방법에 대해 다룹니다.


![캡처](https://user-images.githubusercontent.com/37654013/85227510-f58b8e00-b418-11ea-83fa-6ca3ec55a375.JPG)


AI는 분명히 우리 사회속에 많은 문제를 해결해주고 있지만 그와 동시에 많은 부작용도 발생하고 있습니다. 논문에 나온 예시를 예로 들자면 COMPAS 시스템에 의하면 흑인의 재범률이 실제 그들의 재범률보다 높게 책정되는 경우(인종차별) 그리고 Google 광고의 경우 고소득 직업에 대한 광고를 여자에게는 남자보다 덜 노출되는 경우(성차별)가 있습니다. 그외에도 사실 다양한 경우가 많습니다. 

"Bias is as old as human civilization" 처럼 편견은 새롭게 생긴 문제가 아닌 사람이 서로 함께 살게되며 자연스럽게 생긴 문제입니다. 그러나 단지 이 편견이 있어서 문제가 아닌 AI에서 일어나는 문제는 이 편견을 더 과장하고 새로운 편견을 만들어낸다는 것입니다. 인공지능은 만병통치약이 아닙니다. 그렇기 때문에 모든걸 단지 기술적으로만 해결할 수 없고 이러한 문제점을 인식하는 것은 인공지능을 공부하며 필수적 요소라고 생각합니다. 

게다가 최근에는 이런 편견에 대한 문제뿐만 아니라 인공지능이 사회에 미치는 영향에 대한 걱정으로 연구의 길을 떠나는 사람도 심심치않게 볼 수 있습니다. 가장 큰 예로는 Joseph Redmon이 있습니다. Computer Vision을 공부하는 사람 중에 모르는 사람이 많이 없을 정도로 많은 영향을 미친 YOLO의 저자입니다. 이번 논문에 대한 포스팅이 끝난후에는 편견에 대한 내용이 아닌 사회에 미치고있는 부정적인 영향에 대한 내용과 그에 대한 규제에 대해서로 다뤄볼 예정입니다.

![J](https://user-images.githubusercontent.com/37654013/85228168-caa33900-b41c-11ea-8dbd-95351e06c3e7.JPG)


