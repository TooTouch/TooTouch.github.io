---
title: "Bias in Data-driven AI Systems -
An Introductory Survey - Introduction"
categories: 
    - Setting
toc: true
---

# 시작말

오늘도 또다시 읽어보고 싶은 논문들을 줍줍하다가 그간 관심있게 생각했던 인공지능 속 Bais에 대한 논문을 알게되서 글도 읽고 정리도 해볼겸 글을 작성하게 되었습니다. 

논문 제목은 "Bias in Data-driven AI Systems -
An Introductory Survey" 이고 올해인 2020년 초에 나온 논문입니다. 저자들은 모두 유럽쪽 인사들이였고 내용에서 또한 인공지능에 대한 규제에 대해서 유럽에 상황들과 엮어서 소개하고 있습니다. 그러나 인공지능에 대한 규제는 단순히 유럽에서만은 문제는 아니기에 꼭 제한적인 내용은 아니라 생각됩니다. 

아래 이미지에서 볼 수 있듯이 논문의 내용은 크게 세 가지로 구성되어 있습니다. 따라서 포스팅도 세 번에 나눠서 작성하려 합니다.

1. **Understanding bias** - bais가 우리 사회속에 어떻게 생기고, 어떻게 사회-기술적 체제(socio-technical systems)에 들어오는지 설명하고 공정함(fairness)에 대한 정의를 다룹니다.
2. **Mitiating bias** - AI가 의사결정을 하는 단계마다 (pre-processing, in-processing 그리고 post-processing 추가로 데이터, 학습 알고리즘 그리고 모델 결과) bias를 다루는 방법을 다룹니다.
3. **Accounting for bias** - AI의 의사결정을 사람이 이해할 수 있는 방법으로 설명하여 사전에 bais 대처할 수 있는 방법에 대해 다룹니다.


![캡처](https://user-images.githubusercontent.com/37654013/85227510-f58b8e00-b418-11ea-83fa-6ca3ec55a375.JPG)


AI는 분명히 우리 사회속에 많은 문제를 해결해주고 있지만 그와 동시에 많은 부작용도 발생하고 있습니다. 논문에 나온 예시를 예로 들자면 COMPAS 시스템에 의하면 재범률이 흑인이 실제 그들의 재범율보다 높게 책정되는 경우(인종차별) 그리고 Google 광고의 경우 고소득 직업에 대한 광고를 여자에게는 남자보다 덜 노출되는 경우(성차별)가 있습니다. 그외에도 사실 다양한 경우가 많습니다. 
