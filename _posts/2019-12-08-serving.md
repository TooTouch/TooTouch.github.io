---
title:  "대충 모델 서빙/경량화에 대한 글"
categories: 
    - Conference Review
toc: true
---

본 글은 2019년 모두의연구소에서 진행하는 모두콘의 강연 내용을 기반으로 작성한 글입니다. 좋은 강연을 해주신 Naver Clova 이혜민님, 모두의 연구소 MoT랩실의 신정아님, Naver Glace 곽도영님 그리고 삼성전자 이진원님께 감사의 말씀드립니다.

# 시작말

이번 2019년 모두콘을 통해서 처음 **모델 서빙**과 **경량화**에 대해 알아볼 수 있었다. 컨퍼런스에 가서 그동안 공부하던 내용을 듣는것보다 공부는 해야하는데 시간내서 듣기에는 아직 좀 그렇고 뭔가 찜찜한데 설명해줄 사람이 없어서 아쉬웠던 부분을 채울 수 있어서 좋았다. 

# 모델 서빙 방법

모델을 서빙하기위한 방법은 크게 두 가지이다. 

1. *On-device* 
2. *Server*

**On-device**는 말그대로 디바이스위에서 동작하는 서비스이다. **Server**의 경우는 데이터를 서버로 보내서 출력값을 다시 돌려받는 형식의 서빙방법이다. 여기서는 On-device의 경우를 얘기해보기로 한다.

On-device 서빙을 위해서 도움이 되는 프레임워크들은 대표적으로 **TF Lite, ML Kit, Pytorch Mobile**이 있다. 마찬가지로 Server의 경우 **GCP, ML Kit**이 있다. 

# 모델 서빙을 위해 고려할 사항

모델을 서빙하기 전에 여러가지 고려해야할 사항들이 있다. 이에 대해서는 모든 것을 다 고려하면 좋지만 그러기는 어렵기때문에 우선순위를 정하고 시작하는 것이 중요하다. 아래는 모델 서빙을 위한 5가지 고려할 사항들이다.

1. *추론 속도* 
2. *모델 크기* 
3. *메모리 사용량* 
4. *베터리 사용량*
5. *정확도*

# 모델 경량화를 위한 방법

모델 서빙을 할때 모델 크기를 줄이는 것은 굉장히 중요한 일이다. 특히 딥러닝의 경우 파라미터 수가 백만개보다 훨씬 많은 경우가 대부분이고 정확도를 높이기위해서 모델의 크기가 점점 더 커지고 있다. 그러나 핸드폰같은 디바이스에서 작동하도록 하기위해서는 모델 사이즈를 최대한 줄여서 부담을 줄여야한다. 구글 스토어의 경우는 앱크기가 100mb로 제한이 있다고한다. 

모델 경량화를 위한 방법으로는 크게 두 가지로 먼저 구분할 수 있다.

1. *Reduce size of operands for storage/compute* 
    - Quantization
    - Floating point → Fixed point
2. *Reduce number of operations for storage/comput* 
    - Network Pruning
    - Knowledge Distillation
    - Compact Network Architecture

첫 번째를 예를 들자면 **Quantization**이다. float32를 uint8로 줄이는 것과 같이 부동소수점을 특정수로 줄이는 방법이다.  또는 [Floating point를 Fixed point](https://m.blog.naver.com/PostView.nhn?blogId=zzbksk&logNo=221016046702&proxyReferer=https%3A%2F%2Fwww.google.com%2F)로 바꾸는 방법도 있다. 

두 번째의 경우는 **가지치기(Pluning)**를 통해 필요한 노드들만 연결하는 방법이 있다. 잠시 딴얘기로는 이 Plunning의 경우 아직 Tensor Core에서는 작동이 안되기때문에 TPU 사용이 안된다고한다. **Knowledge Distillation**은 Teacher-Student Network라고 불리기도 한다. 앙상블 모델이나 성능이 좋게 나온 큰 모델을 통해서 보다 가볍고 단순한 모델을 가르치듯히 학습하는 방법이다. 이 방법은 2015년에 Geoffrey Hinton과 google 연구원 둘이 함께 쓴 [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf)에서 처음 등장했다. 학습 방법은 softmax를 거친 확률값을 활요하는 것인데 자세한 내용은논문을 참고하길 바란다. 남은 하나는 **모델 구조의 경량화**에 대한 내용인데 이에 대해서는 자세히 다뤄보도록 한다.

모델 구조에서 경량화를 위한 방법은 크게 다섯 가지로 다뤄볼 수 있다.

1. *Convolution*
2. *Filter Factorization*
3. *Group Convolution*
4. *Depthwise Separable Convolution*
5. *Feature Map Reuse*

**Convolution**의 경우 대표적으로 **bottleneck layer**나 **Global Average Pooling(GAP)**이 있다. Bottleneck layer의 경우 ResNet 논문에서 나왔으며 1x1 convolution을 통해서 적은 파라미터로 channel 수를 맞춰주는 역할을 한다. GAP는 FCN 대신 사용되면서 수많은 파라미터를 줄일 수 있었다.  
[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)

<p align="center">
    <img src='https://drive.google.com/uc?export=view&id=1THErKYLGLv3jSvDlrhyVnFVJqe-D_hdb' width='500'/>
</p>

**Filter Factorization**의 경우 VGG 논문에서 3x3 convolution filter로 통일시킨 이유와 같이 5x5 filter를 한번 하는 것보다 3x3 filter를 두번하는게 출력사이즈는 동일하지만 더 좋은 성능을 나타낸다고 한다. 

**Group Convolution**은 입력채널을 여러개의 그룹으로 나누어서 각각 서로다른 독립적인 convolution 연산을 수행하는 것을 말한다. Alex Net이 이와 같은 방법을 사용했다. 실제 논문에서는 그런 언급은 없었고 의도한게 아니였지만 우연히 좋은 결과를 만들었다. 원래의도는 2개의 GPU의 사용으로 두 갈래로 나뉘었지만 이게 실제로 Group Convolution 역할을 하게 되었다. Alex Net의 경우 한 개로만 했을때 더 성능이 떨어졌다고 한다.  
[ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

<p align="center">
    <img src='https://drive.google.com/uc?export=view&id=1Y0lYObI1WfeB2E5895aLhUhTpHX6lnRt' width='700'/>
</p>

**Depthwise Separable Convolution**은 MobileNet에도 사용된 방법이다. Xception 논문에서 처음 소개되었고 내용은 아래 그림과 같다. 예를 들자면 8개의 채널을 가진 입력값에 3x3 convolution filter로 16개의 채널을 가진 출력값을 원한다면 기존에 계산한 방식은 16개의 3x3 filter로 8개의 채널을 모두 연산해야한다. 그렇다면 총 8x16x3x3 = 1,152 개의 매개변수가 필요하다. 그러나 Xception에서 제안한 방법을 사용하면 1개의 3x3 filter로 입력값과 같은 채널로 출력값을 만들고 16개의 1x1 filter로 출력값에 대해 연산하여 16개의 채널을 가진 새로운 출력값을 만든다. 그 결과 (8x3x3) + (8x16x1x1) = 200 개의 매개변수로 이전에 비해 훨씬 적은 연산을 요구한다.  
[Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357.pdf)

<p align="center">
    <img src='https://datascienceschool.net/upfiles/3356051ff4004f3e8a2554647219e0bd.png' width='700'/><br>
    <i>Source: <a href="https://datascienceschool.net/view-notebook/0faaf59e0fcd455f92c1b9a1107958c4/">DataScienceSchool</a></i>
</p>

**Feature Map Reuse**는 DenseNet에 사용된 방법을 말한다. 말그대로 재사용이다. DenseNet 구조를 보면 이전에 사용한 channel의 나중에 쌓고 또 convolution하고 나중에 또 쌓고 convolution하는 과정을 갖는다. 이를 통해서 불필요한 파라미터를 추가하는 것을 방지하고 성능또한 좋다는 결과를 내었다.  
[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)

<p align="center">
    <img src='https://drive.google.com/uc?export=view&id=1kFD8vMl8xGgrfgNAYGkaMkji7cmsWcN1' width='500'/>
</p>

# 모델 경량화를 평가하는 방법

경량화를 진행했다면 실제 얼마나 영향이 있는지 확인해야하는 과정이 필요하다. 일반적으로 생각하기에는 연산량이 줄으면  속도는 올라갈것으로 예상한다. 그러나 꼭 그렇지는 않다. AlexNet과 SqueezeNet을 비교하자면  [SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE](https://arxiv.org/pdf/1602.07360.pdf) 과 같이 논문 제목에서 보여지듯 SqueezeNet은 AlexNet보다 훨씬 적은 파라미터를 가지고 있다. 그러나 추론시간에 대해서 보자면 AlexNet이 SqueezeNet보다 더 빠르다. 그 이유는 Layer의 개수차이이다. SqueezeNet이 더 깊은 레이어를 가지고 있기때문에 추론시간이 더 걸리는 것이다. 

때문에 보다 정확한 방법으로 경량화에 대한 평가를 진행하고자 올해(2019) ICIAR에서 **NetScore**라는 평가방법을 제안하는 논문이 나왔다.  
[NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage](https://arxiv.org/pdf/1806.05512.pdf)

<p align="center">
    <img src='https://drive.google.com/uc?export=view&id=1xkE-b2fXBmr_U9yr8U-dz6k0tcC75vtR' width='100'/>
</p>


위 식에서 D(N)은 **Information Density**를 뜻한다. a(N)은 모델의 정확도 그리고 p(N)은 파라미터의 수를 말한다. 

<p align="center">
    <img src='https://drive.google.com/uc?export=view&id=1beJo9WKvB3bIrnz9RX07LSgJ-ryaUmla' width='200'/>
</p>

그리고 위의 식이 바로 논문에서 제안하는 NetScore이다. alpha, beta, gamma는 순서대로 정확도의 영향력, 모델 복잡도 그리고 연산량을 나타낸다고 한다. m(N)은 모델의 추론시간동안 계산된 multiply-accumulate(MAC)의 횟수라고 한다. 

결과적으로 ILSVRC 2012 데이터에 대해 Top-1 accuracy, Information Density 그리고 NetScore를 평가했을때 정확도는 조금 떨어졌지만 **SqueezeNet 1.0**이 가장 경량화에서 좋은 결과를 내었다.

# 모바일 서빙 개발자의 입장

모바일 개발 프로세스는 크게 **ML 영역**과 **모바일앱 영역**이 있다. 두 과정은 순차적으로 이루어지기보다 병렬적으로 진행하는것이 가장 이상적인 방법이다. 때문에 모바일 서빙을 하기위해 모바일 개발자와 모델 개발자는 서로 커뮤니케이션이 원활하게 이루어져야한다. 두 집단간의 협력 포인트는 바로 당연한걸 당연하게 넘어가지않고 친철하게 설명해주는 것이다. (이건 사실 여기에만 해당하는 것이아닌 서로 다른 부서간 협력할때 필요한 예의라고 생각된다.)

예를 들면 아래와 같이 나열해 볼 수 있다.

- 입출력 형태 (shape)
- 전처리와 후처리 과정
- 값의 정상 범위와 의미

입출력 형태는 처음 모델 개발하는 사람들도 굉장히 헷갈려하는 부분이다. 때문에 모바일 개발자 입장에서 입출력 형태에 대한 사전 지식이 없다면 개발과정에서 오해가 많이 발생한다. 전처리와 후처리 과정을 정확하게 알려주는 것 또한 중요하다. 어떤 과정이 있는지 알고 개발과정에서 보다 효율적으로 개발할 수 있다. 값의 정상 범위와 의미를 아는 것 또한 디버깅을 하기위한 이해과정이다.

# 맺음말

현재 계획하고있는 프로젝트의 목표 중 서비스도 있기때문에 서빙에 대한 공부를 꼭 하고 싶었는데 마침 모두콘에 서빙과 경량화에 대한 세션이 연달아 있어서 모두 들을 수 있었다. 좋은 기회였고 앞으로도 더 많은 공부와 구현을 해나갈 예정이다. 

마지막으로 좋은 내용 전달해주신 연자님들에게 다시 한번 감사의 말씀을 드립니다.