---
title: "KDX 소비트렌드 시각화 대상 후기 Part 2"
categories: 
    - Contest Review
toc: true
---

지난번에는 [KDX 소비트렌드 시각화 대상 후기 Part 1: 참여과정](https://tootouch.github.io/contest%20review/kdx_dashboard_part1/)에 대해 썼다면 이번에는 **공모전 내용**을 위주로 작성해보려 한다. 아쉬운 점은 사용한 데이터와 제출한 자료의 저작권 문제로 원본이 아닌 예시로 대신 작성하였다.

공모전 목적은 소비 트렌드를 나타낼 수 있는 대시보드를 제작하는 것이었다. 따라서 이번 글에서는 소비 트렌드를 나타내기 위해 어떤 데이터를 사용했고 어떤 방법을 적용해서 어떤 식으로 시각화했는지 작성해보려 한다. 

# 시각화란

내가 생각하는 이상적인 시각화는 **'단순한 게 최고다'**이다. 무엇보다 시각화는 직관적이어야 한다. 그리고 그 대상은 일반인이다. 전문가만 볼 수 있는 시각화는 시각화하는 의미가 없다고 생각한다. 무엇보다 시각화란 주어진 정보를 **짧은 시간 안에 효율적으로 전달**하고 테이블 형태로는 보기 어려운 **인사이트를 찾는 것**에 있다. 

## 그래프의 기본 요소

또한 그래프를 작성하면서 기본만 갖춰도 충분히 좋은 시각화를 할 수 있다. 내가 생각하는 그래프의 기본 구성이라 한다면 **제목, 축 레이블, 척도, 글씨 크기, 색상** 정도가 있다. 만약 이 기본이 갖춰져 있지 않다면 아주 간단한 그래프여도 무엇을 나타내는지 알기가 쉽지 않다. 음.. 그냥 넘어갈까 했지만, 예시를 만들어 봐야겠다. 물론 이 정도는 이미 알고 있는데!? 이렇게 기본적인 걸 안 지키는 사람이 있나? 싶을 수 있지만 내 경험상 꽤 많았다. 물론 나도 깜빡하고 넘어가는 경우가 아직 종종 있다.

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101464969-b29b8380-3982-11eb-9bdf-701961c61262.jpg'><br><i>그림 1. 기본 요소가 갖춰지지 않은 그래프 예시</i>
</p>

**그림 1**은 각 기본 요소가 갖춰지지 않음에 따라 어떤 영향이 있는지 예시로 만들어본 그래프이다.


**(ㄱ) : 제목이 없음**

**(ㄱ)**은 제목을 안 적는 경우다. 제목은 항상 중요하다. 제목을 어떻게 작성하는지에 따라 내용이 무엇을 말하고자 하는지 쉽게 상상할 수 있게 한다. 평소 데이터 분포나 여러 결과를 요약할 때 습관처럼 제목을 안 쓰는 경우가 많다. 나만 보는 그래프에는 사실 제목이 크게 필요가 없기 때문이다. 하지만 결과를 공유할 때도 습관이 되어서 제목을 안 붙이는 경우가 많다.

**(ㄴ) : x 축 레이블이 없음**

**(ㄴ)**은 x축의 레이블이 없는 경우이다. 프로그래밍 언어가 무엇인지 또는 어떤 종류가 있는지 아는 사람은 각 범주만 보고도 무엇인지 알기 쉽지만 그렇지 않은 사람은 만약 '프로그래밍 언어'라는 레이블이 없다면 알기 쉽지 않다.

**(ㄷ) : y 축 레이블이 없음**

**(ㄷ)**는 y축에 레이블이 없는 경우이다. 막대그래프에서 y축은 개수를 나타내는 게 일반적이긴 하지만 평균이나 중앙값 등 여러 통계량으로 나타내는 경우도 있다. 때문에 y축 레이블이 없는 경우 이 값이 무엇을 나타내는지 알기 어렵다.

**(ㄹ) : 척도가 없음**

**(ㄹ)**은 척도를 생략한 경우다. 이러한 경우 축마다 나타내는 내용이 무엇인지는 알지만 단지 그 값의 크기만 비교 가능할 뿐이다. 척도 없이는 그 값의 차이가 얼마나 나는지 객관적으로 알기 어렵다. 

**(ㅁ) : 글씨가 작아서 안보임**

**(ㅁ)**은 글씨 크기가 작아서 눈에 힘을 주고 보거나 확대해서 봐야 할 정도이다. 아무리 기본적인 요소를 다 갖추었다고 해도 글씨 크기가 작아서 보지 못할 정도라면 없는 것만 못하다.

**(ㅂ) : 알 수 없는 키워드**

**(ㅂ)**은 분석한 사람 이외에는 알 수 없는 키워드로 작성된 경우이다. 보통 데이터를 나타내는 범주나 특성은 키워드로 저장된 경우가 많다. 때문에 결과를 공유할 때는 반드시 다른 사람도 알기 쉽게 변환해서 작성해야 한다.


<p align='center'>
    <img width='600' src='https://user-images.githubusercontent.com/37654013/101469758-7a973f00-3988-11eb-8462-035d521e6905.jpg'><br><i>그림 2. 그래프 색상 예시</i>
</p>

추가로 **그림 2**와 같이 범주마다 색상도 고려해주면 좋다. **(ㄱ)**은 범주별로 서로 다른 색상을 지정한 것이고 **(ㄴ)**은 막대의 크기에 따라 명암을 조절하였다. 이러한 경우에 두 가지 방법 중 어떤 방법이 더 좋은 방법일까? 정해진 답은 없는 것 같다. 

나 같은 경우는 현재 범주가 세 가지 밖에 없어서 각각 서로 다른 색상인 **(ㄱ)**이 더 나아 보인다. 또한, 색상도 눈에 띄게 차이가 나기 때문에 만약 나중에 '어떤 프로그래밍 언어를 사람들이 가장 많이 썼더라? 생각했을 때 아 빨간 거!' 하고 빨간색을 기억하며 다시 찾아보기 좋아 보인다. **(ㄴ)**의 경우는 만약 범주가 많다면 좋을 것 같다. 범주가 많은 경우 서로 다른 색상으로 나타내는 것에는 한계가 있기 때문에 크기에 따라 색의 진함을 달리해서 가장 진한 색상 위주의 범주가 어떤 것인지 알기 쉬울 것이다. 또한, 범주가 많은 경우 **크기가 큰 순서대로 정렬**하여 보여주는 것이 더 좋다.

# 데이터 

우리 팀이 사용한 데이터는 크게 두 가지였다. 하나는 신한카드에서 제공한 2019년 1월부터 2020년 6월까지의 **오프라인** 카드 사용 데이터이고 다른 하나는 엠코퍼레이션에서 제공한 2019년 1월부터 2020년 6월까지의 **온라인** 구매 데이터이다. 우리는 두 데이터를 활용해서 오프라인과 온라인 각각의 소비 동향을 볼 수 있는 대시보드를 만들었다. 각각 데이터에 대한 정보는 아래와 같다.

**신한카드 오프라인 데이터**

- 데이터 건수: 195,599 건 

**변수** | **설명** 
---|---
일별 | 날짜 (2019.01.01 ~ 2020.06.30)
성별 | F / M
연령대별 | 20대 ~ 70대
업종 | 총 30개 분류 
카드이용건수(천 건) | 전체카드 시장분으로 추정

**엠코퍼레이션 온라인 데이터**

- 데이터 건수: 1,837,833 건

변수 | 설명
---|---
구매일자 | 날짜 (2019.01.01 ~ 2020.06.30)
고객성별 | F / M
고객나이 | 10대 ~ 70대
카테고리명 | 총 64개 분류
구매수 | 해당 일자 구매수
구매금액 | 해당 일자 구매금액
OS유형 | iOS / WINDOWS / 안드로이드

다행히 데이터가 생각보다 잘 정리 되어있는 상태여서 정제하는데 큰 어려움은 없었다. 이후는 앞서 말했듯이... 저작권이 나한테 없기 때문에 간단히 이런 데이터가 있었고 어떤 식으로 분석했는지 그리고 그래프는 예시로 작성한다.

# 군집 분석

각 데이터의 범주별 특성만 가지고 대시보드를 만들기는 너무 심심하다. 성별에 따라 또는 연령에 따라 구매건수는 평균 몇이니.. 표준편차는 몇이니.. 물론 기본적인 통계치를 나타내는 게 나쁜 건 아니지만 심심하다. 시각화한다면 가장 먼저 생각하는 방법론이 군집화(clustering)이지 않나 싶다. 군집 분석은 기계 학습(machine learning) 방법 중 **비지도 학습**에 속한다. 비지도 학습이란 **정해진 정답이 없는 경우**를 말한다. 여기서 정답이란 사람이 달아놓는 어떤 값을 말한다. 예를 들어 아래 같은 경우가 정답이 있는 경우이고 이때 정답은 3가지라고 볼 수 있다. 이렇게 정답을 가지고 학습하는 방법을 **지도 학습**이라고 한다.

**이름** | **특성1** | **특성2** | . . . | **정답(Y)** 
:---:|:---:|:---:|:---:|:---:
허재혁 | 0223 | 남자 | . . . | 학생 
김민주 | 0118 | 여자 | . . . | 학생
박주희 | 0506 | 남자 | . . . | 자영엉자
. . . | . . . | . . . | . . . | . . .
정병훈 | 1115 | 남자 | . . . | 자영업자 

## K-means

군집 방법에는 대표적으로 K-means라는 방법이 있다. K-means는 가장 많이 사용되는 군집 분석 방법이다. 비지도 학습은 정답이 없는 만큼 분석하는 사람의 주관성이 들어가기 마련이다. 예를 들면 몇 개의 군집을 구성할 것인지에 따라 각 군집마다 해석이 제각각 다르게 변할 수 있다. 물론 K를 몇으로 할 것인지에 대한 기준이 어느 정도 있긴 하다. 여기서는 그 얘기를 쓰려고 하는 게 목적이 아니기에 생략한다. K-means를 간략히 설명하자면 아래와 같이 다섯 단계로 볼 수 있다. 

1. k 개의 점을 랜덤하게 뿌린다.
2. 각 점으로부터 주변 관측치와의 거리를 계산한다.
3. 각 점과 가까운 관측치를 군집으로 지정한다.
4. 지정된 군집의 중심점을 계산한다.
5. 2~4번을 군집의 중심점이 변화가 거의 없을 때까지 반복한다.

## Time Series K-means 

K-means는 보통 2차원 데이터에서 많이 사용된다. 예를 들면 **그림 3**과 같이 각 관측치와 속성들을 기준으로 어떤 관측치가 서로 비슷한지 군집 분석을 통해 묶어줄 수 있다. 하지만 우리가 사용할 데이터는 시간축이 추가된 3차원 데이터이다. 때문에 시간축을 고려한 K-means 군집 분석을 사용했다. Time series K-means를 적용하여 시간축을 기준으로 중심점을 추출하면 각 군집별로 시간 축에 따라 중심점의 변화를 알 수 있다. 

<p align='center'>
    <img width='700' src='https://user-images.githubusercontent.com/37654013/101477944-4b39ff80-3993-11eb-860d-ad72adfe7caa.png'><br><i>그림 3. K-means vs Time Series K-Means</i>
</p>

우리는 여기서 군집을 네 가지 기준으로 고려해서 분석했다. 우선 가장 크게 **온,오프라인**으로 나누었고 그 안에서는 각각 **성&연령대별** 그리고 **업종별**로 구분하여 군집 분석을 시행하였다. 

### 예시

Python에는 여러 시계열 데이터를 분석하기 위해 사용할 수 있는 [tslearn](https://tslearn.readthedocs.io/en/stable/)이 있다. Python을 많이 사용해본 사람이라면 익숙한 이름인 **sklearn**을 기반으로 time series를 적용한 패키지이다. 설치는 아래와 같이 **pip**를 통해 간단히 할 수 있다.

```bash
pip install tslearn
```

**Time Series K-means**는 tslearn 덕분에 정말 감사하게도 두 줄이면 학습이 가능하다. 단 입력값에 대해서는 형태를 맞춰주어야 한다. 입력값의 형태는 위에서 언급했다시피 3차원으로 구성되어 있다. 첫 번째 차원은 군집 분석을 하기 위한 **기준값**이고 두 번째 차원은 **시간** 그리고 세 번째 차원은 **특성값**이다. 즉, **(기준값 x 시간 x 특성)** 이런 식이다.

tslearn에서 작성된 [예제](https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html?highlight=timeserieskmean)를 빌려오자면 아래 같이 간단하게 시계열 군집 분석을 해볼 수 있다.

```python
from tslearn.generators import random_walks
from tslearn.clustering import TimeSeriesKMeans

# generate sample data (50 x 32 x 1)
X = random_walks(n_ts=50, sz=32, d=1)

# clustering
km = TimeSeriesKMeans(n_clusters=3, 
                      metric="dtw", 
                      max_iter=5,
                      random_state=42)
prediction = km.fit_predict(X)                    
```

**TimeSeriesKMeans**에는 여러 파라미터가 있다. 위에서 작성된 예제만 간단하게 설명하자면 아래와 같다.

- **n_clusters**: 군집의 개수
- **metric**: 거리 계산 공식, 세 가지 옵션이 있다. ['euclidean','dtw','soft-dtw']
- **max_iter**: 최대 반복 시행 횟수
- **random_state**: deterministic을 위한 시드값

우리는 위에서 거리 계산 방식으로 **dtw**를 사용했다. dtw가 머야!는 바로 아래에서 설명한다.

## DTW

K-means에서는 각 관측치 간의 거리를 계산하는 과정이 있다. 거리를 계산하기 위해서는 많은 방법이 있다. 그중에서 가장 많이 알려진 방법이 **유클리디안(euclidean) 거리** 이다. 유클리디안 거리 공식은 **식 (1)** 과 같이 각 벡터의 원소 간의 차이를 제곱하여 합한 값이다. 하지만 우리가 사용하려는 시계열 데이터에 유클리디안 거리 공식을 사용하면 여러 한계점이 있다. 예시를 들자면 유클리디안 공식은 **같은 시간을 기준으로 계산**한다는 것이다. 만약 유사한 시계열 분포를 나타내고 있음에도 서로 다른 시간대를 가지고 있다면 유클리디안 거리로 계산한 결과 값은 큰 값을 나타낼 수 있고 이는 **서로 유사하지 않다는 결과**로 이어질 수 있다.

$$
D(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2} \tag{1}
$$

이러한 단점을 보완하고자 사용한 방법은 **Dynamic Time Warping (DTW)** 거리 계산 방법이다[^1]. DTW의 장점은 유클리디안 거리의 한계점을 보완해준다. **그림 4** [^2] 와 같이 거리뿐만 아니라 이전 거리를 함께 고려함으로써 서로 다른 시간대의 시계열 데이터 간의 유사도를 계산할 수 있고 이름에서 나타나듯이 길이가 서로 다르더라도 유사도를 계산할 수 있다는 장점이 있다. 이러한 DTW는 순서(sequence)가 있는 여러 문제에서 사용된다. 예를 들면 발화 속도가 다른 음성 간의 유사도를 고려한 음성 인식이나 유사한 걸음걸이를 계산하는 등 다양한 방면에 사용된다.

<p align='center'>
    <img width='300' src='https://upload.wikimedia.org/wikipedia/commons/6/69/Euclidean_vs_DTW.jpg'><br><i>그림 4. 유클리디안 거리와 DTW 비교 </i>
</p>

논문을 보면 **DTW**을 계산하기 위해서는 **식 (2) 또는 (3)** 중 거리 계산 방법을 먼저 정의한다. **식 (2)**는 두 값의 차이에 절댓값을 취하는 방법이고 **식 (3)**은 두 값의 차이에 제곱을 취한 값이다.

$$
\delta(i, j) = | x_i - y_i | \tag{2}
$$

$$
\delta(i, j) = ( x_i - y_i )^2 \tag{3}
$$

위에서 거리 계산 방법을 정의하였으면 아래 점화식을 통해 누적된 거릿값이 최종적인 DTW 값이 된다.

$$
\gamma(i, j) = \delta(i, j) + min \begin{pmatrix}
                             \gamma(i-1, j) \\
                             \gamma(i-1, j-i) \\  
                             \gamma(i, j-1) \end{pmatrix}
\tag{4}
$$

### 예시

단순히 그림과 식으로는 DTW 계산 과정이 어떻게 되는지 그리고 왜 유클리디안 거리와 차이가 있는지 이해가 어려울 수 있어서 같이 예시를 작성했다. 두 시계열 데이터 **x**, **y**가 있고 **y**는 금방 알 수 있듯이 **x**에 2를 더하고 한 타임씩 뒤로 미룬 값이다. 두 시계열 데이터는 비슷한 분포를 가지지만 서로 다른 시간대를 나타내고 있다.  

$$
x = (1,2,10,5,15) \\
y = (0,3,4,12,7)
$$

<p align='center'>
    <img width='500' src='https://user-images.githubusercontent.com/37654013/101589051-06fa3e00-3a2b-11eb-8910-bb73dec2ce22.jpg'><br><i>그림 5. 두 시계열 데이터 x, y의 분포 비교</i>
</p>

이때 두 시계열 분포 간의 거리를 DTW를 **그림 6**과 같이 표로 직접 계산해서 확인해보면 어떻게 계산되는지 직관적으로 알 수 있다. 거리 계산식은 python에서 구현된 코드와 비교하기 위해 **식 (3)**을 사용하였다. DTW 값은 **78**이 나왔다.

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101588402-ac141700-3a29-11eb-817e-5beec3ed8d99.png'><br><i>그림 6. 식 x, y에 대해 시간마다 DTW 공식을 계산한 예제</i>
</p>

tslearn을 통해 dtw를 계산해보고 **그림 6**에서의 결과와 똑같은 값이 나오는지 확인해보자. 결과 값을 보면  약 8.83176 이 나왔다. 오잉? 하고 결과값 다른데? 라고 생각한다면 그럴 수 있다. 하지만 tslearn에서 적용한 dtw는 결과 값에 제곱근을 취해주었다. 때문에 제곱을 해주면 위에서 계산한 값과 동일함을 알 수 있다.

```python
from tslearn.metrics import dtw_path
import numpy as np

x = np.array([1,2,10,5,15])
y = np.array([0,3,4,12,7])

d = dtw_path(x,y)

print('dtw path: ',d[0])
print('dtw distance: ',d[1])
```
**[output]**  
dtw path:  [(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 4)]  
dtw distance:  8.831760866327848


우리가 사용한 TimeSeriesKmeans에서의 dtw는 사실 위의 dtw에서 조금 개선된 방법이다. 여기서 사용된 옵션 dtw는 **DTW Barycenter Averaging (DBA)** 라는 방법이다. K-means도 사실 여러 한계점이 있는 군집 분석 방법이다. 예를 들어 초깃값이 랜덤하게 배정되는 과정에서 잘못된 곳에 초기값이 지정된다면 **local minima**에 빠지기 쉽다. 때문에 초깃값에 따라 결과 값이 크게 달라질 수 있다는 단점이 있다. **DBA**는 이러한 한계를 개선하고 **견고성(robustness)**과 **수렴성(convergence)**을 나타낼 수 있는 방법이다[^3]. 


# 그래프 구성

대회에서 조건은 대시보드를 최소 다섯 개의 그래프로 구성하는 것이었다. 우리는 크게 네 가지 온,오프라인 그리고 성&연령별 그리고 업종별로 구분했고 각각 아래 그래프를 적용해서 나타냈다.

1. **군집 분포**
2. **특성별 2019년과 2020년 소비트렌드 변화**
3. **특성별 2019년 대비 2020년 구매 건수 변화율**
4. **특성별 2019년 대비 2020년 구매 금액 변화율 (온라인만 해당)**
5. **특성별 OS 사용 비율 (온라인만 해당)**

## 군집 분포

각 군집별 분포를 어떤 식으로 시각화할까 고민을 많이 했다. 시각화하는 데 있어서 매번 막대그래프나 꺾은선 그래프 또는 원 그래프 등 지겨웠다. 그래서 적용한 게 **픽토그램** 이미지로 시각화하는 것이었다. **픽토그램**은 **그림문자** 라고도 불린다. 가장 흔히 볼 수 있는 픽토그램은 비상구 그림, 화장실 그리고 엘리베이터의 열림 또는 닫힘 버튼이 대표적이다. 

**성&연령별**  

성&연령별에 대해 픽토그램을 찾아보다가 적절한 게 없어서 만들었다. 우리 팀에는 그만한 그림실력을 갖춘 능력자가 있으니까! 연령과 성별을 고려하여 10대~70대까지의 남녀에 대한 픽토그램 이미지를 만들었다. 

성&연령별 군집 분포를 시각화한 방법은 **그림 7**과 같이 해당 군집 이외 다른 군집의 투명도를 조절하여 해당 군집이 어떤 분포를 나타내는지 볼 수 있게 하였다. 원래는 남녀 각각 색상 없이 검은색으로 했지만, 색상이 있었으면 더 좋았을 것을.. 이라는 아쉬움에 추가해서 예시를 만들었다.

- [sex_age_plot 함수 코드 확인하기](https://gist.github.com/TooTouch/a7635f1f5fc26f75932c2ab789abbe16)

```python
sample_cluster = [1,2,3]
sex_age_plot(cluster=sample_cluster, save=True)
```

**[output]**

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101610157-36b93e00-3a4b-11eb-8873-4d6c58a6d4c5.jpg'><br><i>그림 7. 성&연령별 군집 분포 예시</i>
</p>

**업종별**  

업종별도 위와같은 방식으로 했지만 수십개의 업종을 픽토그램으로 하는 것은 구분하기 어렵기 때문에 이름표를 만들어주었다. 그리고 각 업종별 구매 건수나 구매 금액에 대해 가시화하기 위해 대분류로 묶어 주었다. **그림 8**처럼 각 대분류별 색상을 지정하고 대분류에 속하는 업종명을 같은 색상으로 구분하였다. 그런 다음 위와 같이 투명도를 조절하여 해당 군집의 업종 분포를 나타냈다.

- [cat_plot 함수 코드 확인하기](https://gist.github.com/TooTouch/b2bdb9fd205ea5a411ede796b12ebf91)

```python
sample_cluster = [0,1,2,6,7,8,13]
cat_plot(cluster=sample_cluster, save=True)
```

**[output]**

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101617791-6caef000-3a54-11eb-8c09-333b9f10d8b0.jpeg'><br><i>그림 8. 업종별 군집 분포 예시</i>
</p>

## 소비트렌드 변화

소비트렌드는 앞서 시계열 군집 분석을 통해 구한 중심점을 통해 2019년과 2020년으로 구분하여 꺾은선 그래프로 시각화하였다. 아래 예시를 들어 설명하자면 우선 기간이 일별로 되어있었기 때문에 2019년 1월 1일부터 2020년 6월 30일까지의 샘플 데이터를 만들어 주었다. 샘플데이터는 tslearn에 있는 **random_walk**를 통해 간단히 시계열 샘플 데이터를 만들어 볼 수 있다. 

샘플 데이터는 14개의 관측치와 5개의 특성값으로 지정하여 만들었다. 우리가 사용한 데이터로 예를 들자면 14개의 성(남녀)&amp;연령별(10대~70대)과 5개의 업종으로 볼 수 있다. 이때 군집 분석의 기준은 관측치인 성&amp;연령별이다. 업종을 기준으로 할 경우 반대로 하면 된다. 샘플 데이터의 차원은 **(14 x 547 x 5)** 이다. 

`sns.set_style("whitegrid")`는 그래프 배경을 설정해주는 것인데 갠취에 맞게 사용하면 된다. 나 같은 경우 꺾은선 그래프를 그릴 때 뒤에 격자(grid)가 있으면 비교하기도 쉽고 좋아서 종종 사용한다.

```python
from tslearn.generators import random_walks
from tslearn.clustering import TimeSeriesKMeans

import seaborn as sns
sns.set_style("whitegrid")

# 2019년 1월 1일 ~ 2020년 6월 30일 일별 기간
date = pd.date_range('2019-01-01','2020-06-30',freq='d')

# generate sample data 2019 and 2020
X1 = random_walks(n_ts=14, sz=365, d=5, random_state=42)
X2 = random_walks(n_ts=14, sz=len(date)-365, d=5, random_state=42)
X = np.concatenate((X1, X2), axis=1)

# clustering
km = TimeSeriesKMeans(n_clusters=4, metric="euclidean", max_iter=5, random_state=0)
prediction = km.fit_predict(X)
```

시계열 군집 분석이 끝난 후 객체로 지정한 `km`을 보면 `cluster_centers_`를 통해 군집별 중심점을 확인할 수 있다. 이때 결과 값의 차원은 **(군집 수 x 시간 x 특성)** 이다.

```python
km.cluster_centers_.shape
```

**[output]**
(4, 547, 5)

시각화를 위해 첫 번째 군집의 두 번째 특성에 대한 변화를 추출하였다. 추출한 결과는 **그림 9**와 같이 기간별 중심점의 차이에 따라 2019년에 비해 얼마나 변화하였는지 알 수 있다.

- [소비 트렌드 코드 확인하기](https://gist.github.com/TooTouch/c3781d0df9e49381d4446132463c239f)

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101629415-a8ea4c80-3a64-11eb-8d67-61736bc7cd30.jpg'><br><i>그림 9. 소비트렌드 변화 시각화 예시</i>
</p>

소비트렌드 변화는 모든 특성에 대해 나타내지 않았다. 월별로 2019년과 2020년의 차이를 계산해서 변화가 큰 다섯 개의 특성을 선택할 수 있도록 대시보드를 구성했다.

## 구매 건수 변화율

**구매 건수 변화율**은 이전 2019년 대비 2020년 얼마나 건수가 변화되었는지를 나타낸다. 군집마다 업종에 따라 얼마나 변화하였는지 나타냈다. **그림 10**과 같이 시각화 방법은 **레이더 차트**를 사용했다. 

**레이터 차트**는 지표마다 중심으로부터 얼마나 퍼져있는지 확인할 수 있는 좋은 지표이다. 막대그래프보다 더 시각적으로 어떤 업종이 영향을 크게 받았는지 비교하기 쉽다. 선택한 군집은 점선 안에 색상을 채워서 나타냈으며 다른 군집과도 함께 비교하기 위해 다른 색상을 지정한 후 점선으로 남겼다. 내부에 있는 y 척도는 2019년 대비 2020년 얼마나 변화했는지를 나타내는 변화율이다. 만약 100%보다 크다면 작년보다 더 구매 건수가 늘었다고 볼 수 있다.

코드와 예시에 사용한 샘플 데이터는 아래 링크에서 확인할 수 있다. 코드 설명은 ... 생략! 간단히만 말하자면 레이더 차트는 matplotlib에 있는 [polar](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.polar.html)로 만들 수 있다. 이때 각 지표에 대해서 표시하기 위해서는 일반적인 그래프에서 x, y를 정하듯 angle과 value를 입력값으로 넣으면 된다.

- [구매 건수 변화율 코드 확인하기](https://gist.github.com/TooTouch/cdeedf1b302882c397668edc43f77d1a)

<p align='center'>
    <img width='500' src='https://user-images.githubusercontent.com/37654013/101634655-5ad94700-3a6c-11eb-83c7-65103de705ed.jpg'><br><i>그림 10. 군집에 따른 업종별 구매 건수 변화율 예시</i>
</p>

## OS 사용 비율

OS 사용 비율은 **그림 11**과 같이 원 그래프를 사용했다. 온라인 데이터에서는 사용자가 구매 시 사용한 OS에 대한 정보가 나타나 있기 때문에 군집마다 위에서 찾은 변화가 큰 특성 5개에 대해 어떤 OS 경로로 구매되었는지 확인했다. 

OS에 대한 사용빈도를 나타낸 이유는 코로나로 인한 온라인 구매가 급증한 상황에서 적절한 마케팅을 위해 어떤 그룹에 적용하는 것이 좋을지 제안하기 위해 사용했다.

- [OS 사용 비율 코드 확인하기](https://gist.github.com/TooTouch/cb2ff423981e664ebbf3878cc3b1c535)

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101725821-af280980-3af4-11eb-8261-6848cb30cac3.jpg'><br><i>그림 11. OS 사용 비율 예시</i>
</p>

## ipywidgets

이번 공모전에서는 대시보드를 만드는 게 핵심이었다. 단순히 그래프를 나열하는 것보다 원하는 정보를 원하는 내용에 맞게 보여줄 수 있도록 동적으로 나타내는 게 좋겠다고 생각했다. 하지만 주어진 분석환경 특성상 여러 동적 그래프나 대시보드를 위한 레이아웃을 구성하는데 많은 제한이 있었다. 때문에 사용한 것이 **[ipywidgets](https://ipywidgets.readthedocs.io)** ! 함수를 찾아보는데 시간이 조금 걸리긴 하지만 ... 익숙해지면 편리하다 익숙해지면...

ipywidgets은 jupyter notebook에서 입력값과 출력값을 상호작용하게 만들어주는 패키지이다. 아래와 같이 간단한 코드만으로 여러 번 입력값 바꿔가면서 `ctrl` + `Enter` 를 쳐가며 셀을 실행시켜 결과 값을 볼 필요가 없다. 아래와 같이 **Dropdown** 으로 군집을 선택할 수 있게 만들었고 선택된 군집에 따라 군집 분포 결과가 바뀌는 것을 알 수 있다.  

각각 결과값은 **VBox**나 **HBox**로 묶어줄 수 있다. 앞글자에서 알 수 있듯이 V(Vertical)는 행으로 H(Horizontal)는 열로 합친다. Vertical과 Horizontal을 맨날 보면서 뭐가 수직이고 수평이지... 하고는 정말 많이 헷갈렸는데 vㅓ티!컬은 뾰족하니까 수직... 호롸라아이즌은 늘어지니까 수평... 으로 외우면 참 좋다. 쿠쿡...

```python
from ipywidgets import widgets, HTML, VBox, Layout, interactive_output

t = HTML('<h1>군집별 분포 예시 - 성별&연령별</h1>', layout=Layout(width='auto', height='auto'))
dropdown_sex_age = widgets.Dropdown(options=range(5), description='군집')
sex_age_interact = interactive_output(sex_age_plot, {'cluster':dropdown_sex_age})

VBox([t, dropdown_sex_age, sex_age_interact])
```

<p align='center'>
    <img src='https://user-images.githubusercontent.com/37654013/101731026-5d847c80-3afe-11eb-911e-5462e41a343a.gif'><br><i>그림 12. ipywidgets을 활용한 예시</i>
</p>

ipywidgets에는 많은 layout 함수들이 있다. 하지만 VBox랑 HBox만 잘 사용해도 적절히 대시보드를 구성할 수 있다. 물론... 다른 대시보드 api들에 비해 허접해 보이지만... 어디까지나 그래 보일 뿐. 내용이 중요하다..! I believe

# 아쉬운점

**[대시보드]**

아쉬웠던 것 중에 가장 큰 것은 **ipywidgets** 말고 더 좋은 대시보드 툴을 사용해보지 못한 것이다. 물론 내가 아는 방법도 제한적이었지만 더 좋은걸 알고계신분은 ... 댓글 달아주시면 ㅎㅎ.. **volia** 도 참 좋은데 사용하려고 했더니 아무래도 terminal을 사용할 수가 없어서 활용할 수 없었다. **nbextension**도 안되고... 아쉽다 ㅠㅠ 시간이 널널했으면 R로 만들어서 **shiny**를 활용하면 더 화려하고 잘 만들어볼 법도 했을 텐데 아쉽다. 그럼 지금 해! 하기엔 나의 의지와 의욕이 점점 줄어들고 있다... 후후...

**[분석]**

데이터를 잠깐 보면서 여러가지 이어보면 좋겠다 싶었는데 아쉬웠다. 특히 소비트렌드 변화에 대해 시계열 모델로 예측도 함께 첨부했으면 좋았을 텐데 두 마리 토끼를 모두 잡으려다 다 놓치기에 십상이기 때문에 적절히 절제했다. 또는 데이터를 더 다양하게 엮어서 활용해보고 싶었는데 그럴 데이터가 있었음에도 그러지 못해 아쉬움이 크다. OS 사용 빈도 관련해서 KDX에서 제공되는 다른 데이터와 엮고 싶었는데 건드리렸다가는 그대로 하루가 날아갈 거 같아서 불가능했다. 

# 맺음말

분명.. 쓴 내용은 최소 A4 10장은 되려나 싶었는데 막상 보니 다 어디 간 거지... 예시를 만들면서 쓰려다 보니 참 어렵다. 그래도 좋은 경험이었고 뉴스까지 나오니 정말 신기한 경험이었다. 당분간은 분석보다는 이제 연구에 집중할 계획이다. 어디까지나 내가 원하는 본업은 연구자기 때문에! 세상에 아주 선한 영향력을 마구마구 뿌리겠단 말이다. 긴 글이지만 다른 시각화에 관심 있는 분들께 도움이 되기를... peace out!


# 참고자료

[^1]: Berndt, D. J., & Clifford, J. (1994, July). Using dynamic time warping to find patterns in time series. In KDD workshop (Vol. 10, No. 16, pp. 359-370).

[^2]: https://commons.wikimedia.org/wiki/File:Euclidean_vs_DTW.jpg

[^3]: Petitjean, F., Ketterlin, A., & Gançarski, P. (2011). A global averaging method for dynamic time warping, with applications to clustering. Pattern Recognition, 44(3), 678-693.