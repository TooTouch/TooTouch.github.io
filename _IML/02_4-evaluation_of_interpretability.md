---
title:  "2.4 Evaluation of Interpretability"
permalink: /IML/evaluation_of_interpretability/
---

해석가능성이 머신러닝에서 무엇을 말하는지 실질적으로 합의된건 없습니다. 이를 어떻게 평가할 것인지도 명확하지 않습니다. 그러나 이 방법에 대해 몇몇 연구들이 진행되어 왔고 아래 문단에서 평가에 대한 몇가지 방법들을 정형화한 시도들에 대해 다루겠습니다.

Doshi-Velez and Kim (2017)는 해석가능성에 대한 평가로 세 가지 방법을 다음과 같이 제안했습니다.

**어플리케이션 수준의 평가**(실제 작업): 제품에 설명을 집어 넣고 사용자에게 평가받는 것입니다. X-ray에서 골절을 찾아내고 표시하는 머신러닝을 구성요소로 갖춘 골절 탐지 소프트웨어를 상상해봅시다. 어플리케이션 수준에서 방사선 전문의들은 골절 탐지 소프트웨어를 직접 테스트해서 모델을 평가합니다. 이 과정을 위해서는 좋은 실험구성과 품질을 평가하는 방법에 대한 이해가 필요합니다. 이를 위한 좋은 기준은 항상 사람이 같은 결정에 대해 얼마나 잘 설명할 것인지 입니다.

**인간 수준의 평가**(단순한 작업)은 어플리케이션 수준의 평가를 단순화한 것입니다. 둘의 차이는 전문가에 의해 실험이 이뤄지지 않는다는 것입니다. 이 방법은 실험을 더 적은 비용으로 할 수 있고(도메인 특히 전문가가 방사능전문의인 경우) 평가자를 찾기가 더 쉽습니다. 예를 들어 사용자에게 서로 다른 설명을 보여주고 어떤게 더 좋은지 고르게하는 것이 있습니다.

**기능 수준의 평가**(프록시 작업)은 사람이 필요하지 않습니다. 이 작업은 모델의 클래스가 이미 인간 수준의 평가에서 누군가에의해 평가된 경우가 가장 효과적입니다. 예로 들자면 최종 사용자가 의사결정나무를 이해할 수 있는 것을 말합니다. 이 경우 설명에 대한 질의 프록시로는 트리의 깊이라고 할 수 있습니다. 더 짧은 트리모형일수록 설명력에 대한 점수는 더 올라갑니다. 이는 트리의 예측 성능이 괜찮고 더 큰 트리모형과 비교했을 때 성능이 크게 떨어지지 않는다면 제한을 두는게 합당하다는 것을 말합니다.

다음 장에서는 기능 수준에서 개별적인 예측값에 대해 설명력을 평가하는 방법에 대해 다룹니다. 평가를 위해 고려해야할 설명의 관련 속성은 무엇일까요?
