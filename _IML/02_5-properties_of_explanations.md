---
title:  "2.5 Properties of Explanations"
permalink: /IML/properties_of_explanations/
---

12:40 AM

우리는 머신러닝 모델의 예측을 설명하고 싶어합니다. 그러기 위해서 우리는 설명을 만들어내는 몇몇 방법들을 사용해야합니다. 설명은 일반적으로 사람이 이해할 수 있는 방식으로 관측치의 특성값을 모델 예측과 관련시킵니다. 다른 유형의 설명은 데이터 관측값들의 집합(예: k-가장 가까운 이웃 모델)으로 구성됩니다. 예를 들어, 서포트 벡터 머신를 사용하여 암 위험을 예측하고 설명으로 의사결정나무를 생성하는 local surrogate  방법을 사용하여 예측을 설명할 수 있습니다. 또는 서포트 벡터 머신 대신 선형 회귀 모델을 사용할 수 있습니다. 선형 회귀 모델은 이미 설명 방법(가중치에 대한 해석)을 갖추고 있습니다.

설명 방법과 설명의 특성을 자세히 살펴보겠습니다(Robnik-Sikonja and Bohanec, 2018[^1]). 이 특성들은 설명 방법이나 설명이 얼마나 잘 나타내는지 판단하는 데 사용될 수 있습니다. 모든 특성이 얼마나 정확하게  측정하는지는 명확하지 않을 수 있으나, 여러 챌린지 중 하나는 바로 계산할 수 있는 방법을 공식화하는 것입니다.

**설명 방법들에 대한 속성들**

- **표현력(Expressive Power)**은 설명 방법이 만들어낼 수 있는 설명의 "언어" 나 구조를 말합니다. 설명 방법은 IF-THEN 규칙, 의사결정나무, 가중치 합, 자연어 또는 다른 것을 생성할 수 있습니다.
- **반투명성(Translucency)**는 설명 방법이 파라미터같이 머신러닝 모델을 조사하는 데 얼마나 표현될 수 있는지를 나타냅니다. 예를 들어 선형 회귀 모형(model-specific)과 같이 본질적으로 해석 가능한 모델에 의존하는 설명 방법은 매우 반투명하다고 볼 수 있습니다. 입력값을 바꾸고 예측치를 관찰하는 것에만 의존하는 방법은 반투명성이 없다고 할 수 있습니다. 시나리오에 따라 다른 수준의 반투명성이 더 좋을 수 있습니다. 무슨 말인가 하면, 높은 반투명성의 장점은 이 방법이 설명을 생성하기 위해 더 많은 정보에 의존할 수 있다는 것입니다. 반투명성이 낮다는 장점은 설명 방법이 휴대성이 높다는 점입니다.
- **휴대성(Portability)**은 설명 방법을 사용할 수 있는 머신러닝 모델의 범위를 나타냅니다. 반투명성이 낮은 방법은 머신러닝 모델을 블랙박스로 취급하기 때문에 휴대성이 더 높습니다. Surrogate 모델은 휴대성이 가장 높은 설명 방법일 수 있습니다. RNN과 같은 경우에만 작동하는 방법은 휴대성이 낮다고 할 수 있습니다.
- **알고리즘 복잡성(Algorithmic Complexity)**은 설명을 생성하는 방법의 계산 복잡성을 설명합니다. 이 속성은 연산 시간이 설명을 생성하는 데 병목 현상인 때를 고려하는 것이 중요합니다.

**개별적인 설명에 대한 속성들**

- **정확도(Accuracy)**: 설명은 보지못한 데이터를 얼마나 잘 예측할까? 머신러닝 모델 대신 예측에 대한 설명을 사용하는 경우 높은 정확도가 특히 중요합니다. 그러나 머신러닝 모델의 정확도 또한 낮을 경우, 그리고 블랙 박스 모델이 무엇을 하는지 설명하는 것이 목표일 경우에는 낮은 정확도도 괜찮습니다. 이 경우에는 충성도만이 중요합니다.
- **충성도(Fidelity)**는 다음과 같습니다. 설명은 블랙 박스 모델의 예측과 얼마나 비슷할까? 높은 충성도는 설명의 가장 중요한 특성 중 하나입니다. 낮은 충성도를 가진 설명은 머신러닝 모델을 설명하는데 무용지물이기 때문입니다. 정확도과 충성도는 밀접한 관련이 있습니다. 블랙 박스 모델의 정확도가 높고 설명의 충실도가 높은 경우 설명도 정확도가 높습니다. 일부분에 대한 설명은 지역적 충실도만을 나타냅니다. 즉, 데이터의 하위 집합(예: local surrogate model) 또는 개별 데이터 관측치(예: Shapley Values)에 대한 모델 예측이라고 할 수 있습니다.
- **일관성(Consistency)**: 동일한 문제에 대해 학습된 모델이 있고 모두 유사한 예측을 생성하는 모델 간의 설명은 어느 정도 차이가 있는가? 예를 들어, 서포트 벡터 머신과 선형 회귀 모형을 동일한 문제로 학습하고 둘 다 매우 유사한 예측을 생성한다고 합시다. 이떄 제가 선택한 방법을 사용하여 설명을 계산하고 설명이 얼마나 다른지 분석합니다. 설명이 매우 유사하다면, 설명은 매우 일관성이 있다고 할 수 있습니다. 그러나 이 특성은 잘 확인해야합니다. 두 모델이 서로 다른 특성을 사용했지만 비슷한 예측(“Rashomon Effect”라고도 함)을 얻을 수 있기 때문입니다. 이 경우 설명이 매우 달라야 하므로 일관성이 높은 것은 바람직하지 않습니다. 모델이 실제로 유사한 관계에 의존하는 경우 높은 일관성을 유지하는 것이 좋습니다.
- **안정성(Stability)**: 유사한 관측값에 대한 설명은 얼마나 비슷할까? 일관성은 모델 간의 설명을 비교하지만 안정성은 모델은 고정하고 유사한 관측치 간의 설명을 비교합니다. 안정성이 높으면 관측치의 특성이 약간 변동하더라도 설명이 크게 바뀌지 않습니다(약간의 변화에도 예측값이 크게 변경되지 않는 한). 안정성이 부족하면 설명 방법의 차이가 클 수 있습니다. 즉, 설명 방법은 설명할 관측값의 특성값이 약간 변경되는 경우에 강한 영향을 받습니다. Local surrogate 방법에서 사용하는 것과 같은 데이터 샘플링 단계와 같은 설명 방법의 비결정적(non-deterministic) 구성 요소에 의해 안정성이 떨어질 수 있습니다. 안정성은 높을 수록 좋습니다.
- **이해가능성(Comprehensibility)**: 사람은 설명을 얼마나 잘 이해할 수 있을까? 이것은 많은 속성들 중 하나인것처럼 보이지만, 방안에 있는 코끼리입니다(사실은 가장 중요한 속성입니다, 원문의 표현을 따르자면). 정의하고 측정하기는 어렵지만 동의를 얻는것은 매우 중요합니다. 많은 사람들이 이해가 가능한지는 대중에게 달려있다는 것에 동의합니다. 이해가능성을 측정하기 위한 아이디어에는 설명의 크기 측정(선형 모델에서 0이 아닌 가중치를 가진 특징의 수, 의사결정 규칙의 수 등) 또는 설명에서 사람들이 머신러닝 모델의 동작을 얼마나 잘 예측할 수 있는지 테스트하는 것이 포함됩니다. 설명에 사용된 특성의 이해성도 고려해야 합니다. 특성을 너무 복잡한 변환하면 원래 특성에 비해 이해하기가 더 어려울 수 있습니다.
- **확실성(Certainty)**: 설명에는 머신러닝 모델의 확실성이 반영되어 있는가? 많은 머신러닝 모델은 예측이 올바르다는 모델 신뢰도에 대한 설명 없이 예측만 합니다. 모델이 한 환자의 암 발생 확률을 4%로 예측하는 경우, 다른 특성값을 가진 다른 환자가 받은 확률만큼 확실할까요? 모델의 확실성을 포함한 설명은 매우 유용합니다.
- **중요도(Degree of Importance)**: 설명은 특성의 중요성이나 설명의 일부분을 얼마나 잘 반영하고 있을까? 예를 들어, 의사결정 규칙이 각 예측에 대한 설명으로 생성되는 경우를 말합니다.
- **새로움(Novelty)**: 설명해야 할 데이터 관측치가 학습 데이터의 분포에서 멀리 떨어진 "새로운" 영역에서 왔는지에 대한 여부를 반영하고 있을까? 이 경우 모델이 정확하지 않고 설명이 무용지물이 될 수 있습니다. 참신함의 개념은 확실성의 개념과 관련이 있습니다. 참신함이 높을수록 데이터의 부족으로 인해 모형의 확실성이 낮아질 가능성이 높습니다.
- **대표성(Representativeness)**: 설명을 나타낼 수 있는 관측치가 얼마나 되는가? 설명은 전체 모델(예: 선형 회귀 모형의 가중치 해석)을 다루거나 개별 예측(예: Shapley Values)만 나타낼 수 있습니다.

---

[^1]: obnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning. Springer, Cham. 159-175. (2018)