---
title:  "5.1 Partial Dependence Plot (PDP)"
permalink: /IML/partial_dependence_plot/
toc: true
---

# Partial Dependence Plot (PDP) 

Partial Dependence Plot(줄여서 PDP 또는 PD 그림)은 머신러닝 모델의 예측 결과에 대해 하나 또는 두 개의 특성이 갖는 한계 효과(marginal effect)를 보여줍니다(J. H. Friedman 2001[^1]).
부분 의존도 그림에서는 대상과 특성 사이의 관계가 선형인지, 단조로운지 또는 더 복잡한지 여부를 표시할 수 있습니다.
예를 들어 선형 회귀 모형에 적용되는 경우 부분 의존도 그림은 항상 선형 관계를 표시합니다.

회귀에 대한 부분 의존 함수는 다음과 같이 정의됩니다.

$$\hat{f}_{x_S}(x_S)=E_{x_C}\left[\hat{f}(x_S,x_C)\right]=\int\hat{f}(x_S,x_C)d\mathbb{P}(x_C)$$

$$x_S$는 부분 의존성 함수를 표시해야 하는 특성이며 $x_C$는 머신러닝 모델 $\hat{f}$에서 사용되는 다른 특성입니다.
일반적으로 집합 S에는 하나 또는 두 개의 특성만 있습니다.
S의 특성은 예측에 미치는 영향을 알고자 하는 특성입니다.
특성 벡터 $$x_S$와 $$x_C$$를 결합하면 총 특성 공간 x가 됩니다.
부분 의존성은 설정된 C의 특성의 분포보다 머신러닝 모델 출력을 무시하여 작동하므로, 함수에는 우리가 관심 있는 집합 S의 특성과 예측된 결과 사이의 관계가 표시됩니다.
다른 특성을 무시하면 S의 특성에만 의존하는 함수를 얻을 수 있습니다. 포함된 다른 특성과 상호 작용합니다.

부분 함수 $$\hat{f}_{x_S}$$은(는) 몬테카를로 방법이라고도 하는 학습 데이터의 평균을 계산하여 추정합니다.

$$\hat{f}_{x_S}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x^{(i)}_{C})$$

부분 함수는 S 특성의 주어진 값에 대해 예측에 대한 평균 한계 효과가 무엇인지 알려줍니다.
이 공식에서 $$x^{(i)}_{C}$$는 관심 없는 특성에 대한 데이터 집합의 실제 특성 값이며, n은 데이터 집합의 관측치 수입니다.
PDP의 가정은 C의 특성이 S의 특성과 상관관계가 없다는 것입니다.
이 가정이 위반되면 부분 의존도 그림에 대해 계산된 평균에는 가능성이 매우 낮거나 심지어 불가능한 데이터가 포함됩니다(단점 참조).

머신러닝 모델이 확률을 출력하는 분류 문제의 경우 부분 의존도 그림은 S의 특성에 대해 다른 값이 주어진 특정 클래스에 대한 확률을 표시합니다.
여러 클래스를 처리하는 쉬운 방법은 클래스당 한 줄 또는 플롯을 그리는 것입니다.

부분 의존도는 글로벌 방법입니다.
이 방법은 모든 관측치를 고려하며 예측된 결과와 특성의 전역 관계에 대한 설명을 제공합니다.


**범주형 특성**

지금까지 우리는 연속형 특성만 고려했습니다.
범주형 특성의 경우 부분 의존성을 매우 쉽게 계산할 수 있습니다.
각 범주에 대해 모든 데이터 관측치에 동일한 범주를 강제로 적용하여 PDP 견적을 얻습니다.
예를 들어 자전거 대여 데이터 집합를 살펴보고 해당 계절의 부분 의존도 그림에 관심이 있다면 각 계절마다 하나씩 4개의 숫자가 표시됩니다.
"여름"의 값을 계산하기 위해 모든 데이터 관측치의 기간을 "여름"으로 대체하고 예측의 평균을 산출합니다.


# 예시

실제로 한 특성는 2D 플롯을 생성하고 두 특성는 3D 플롯을 생성하기 때문에 특성 S 집합에는 대개 하나의 특성 또는 최대 2개만 포함됩니다.
그 이상의 모든 특성을 표현하는 것은 상당히 까다롭습니다.
2D 종이나 모니터에서의 3D도 이미 어려운 작업입니다.

[특정일에 대여할 자전거의 수를 예측하는 회귀 예제로 돌아가 보겠습니다](https://tootouch.github.io/IML/bike_rentals/).
먼저 머신러닝 모델을 적합시킨 다음 부분 의존성을 분석합니다.
이 경우 자전거의 수를 예측하고 부분 의존도를 이용해 모델이 학습한 관계를 시각화하기 위해 랜덤 포레스트를 사용했습니다.
날씨 특성이 자전거 예상 수에 미치는 영향은 다음 그림으로 시각화했습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/pdp-bike-1.png'><br>
    <i>그림 5.2: PDP는 자전거 수 예측 모델과 온도, 습도 및 풍속을 위한 것입니다. 온도에서 가장 큰 차이를 볼 수 있습니다. 뜨거울수록 자전거는 더 많이 빌립니다. 이 추세는 섭씨 20도까지 올라갔다가 30도에서 약간 평평해지고 떨어집니다. x축의 표시는 데이터 분포를 나타냅니다.</i>
</p>

이 모델은 따뜻하지만 너무 덥지 않은 날씨에 평균적으로 많은 수의 자전거를 빌릴 것으로 예측하고 있습니다.
잠재적 자전거 이용자들은 습도가 60%를 초과할 때 자전거를 빌리는 것이 점점 더 금지되고 있습니다.
게다가, 바람이 많이 불수록 사람들이 사이클을 덜 좋아하기 때문에 말이 되지요.
흥미롭게도, 풍속이 25km에서 35km/h로 증가할 때 자전거 대여 예상 수는 감소하지 않지만, 학습 데이터는 많지 않기 때문에, 머신러닝 모델은 아마도 이 범위에 대한 의미 있는 예측을 배울 수 없었을 것입니다.
적어도 직관적으로, 저는 특히 풍속이 매우 높을 때, 풍속이 증가함에 따라 자전거의 수가 줄어들 것으로 예상합니다.

범주형 특성을 가진 부분 의존도를 설명하기 위해 계절 특성의 예측 자전거 대여에 대한 영향을 조사합니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/pdp-bike-cat-1.png'><br>
    <i>그림 5.3: 자전거 수 예측 모델과 계절을 위한 PDP입니다. 예기치 않게 모든 계절이 모델 예측에 유사한 효과를 나타내며, 봄에만 모델이 자전거 대여를 줄일 것으로 예측합니다.</i>
</p>

또한 [자궁경부암 분류](https://tootouch.github.io/IML/cervical_cancer/)에 대한 부분 의존도도 계산합니다.
이번에는 위험요인에 따라 여성이 자궁경부암에 걸릴지 예측하기 위해 랜덤 포레스트를 사용했습니다.
랜덤 포레스트에 대한 다양한 특성에 대한 암 확률의 부분 의존도를 계산하고 시각화합니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/pdp-cervical-1.png'><br>
    <i>그림 5.4: 호르몬 피임약을 사용한 나이와 연도에 근거한 암 확률의 PDP입니다. 연령의 경우 PDP는 40까지 확률이 낮으며 이후로는 증가한다는 것을 보여줍니다. 호르몬 피임약을 복용하는 기간이 길어질수록 특히 10년 후에는 암 발병률이 더 높아집니다. 두 특성의 경우 모두 값이 큰 데이터가 많이 없었기 때문에 PD 추정치는 해당 영역에서 신뢰성이 떨어집니다.</i>
</p>

또한 두 가지 특성의 부분 의존성을 한 번에 시각화할 수 있습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/pdp-cervical-2d-1.png'><br>
    <i>그림 5.5: PDP는 암 발생가능성과 나이와 임신 횟수의 상호작용을 나타냅니다. 이 그림은 45세의 암 발생 확률을 보여줍니다. 25세 미만에서는 1-2명의 임신을 한 여성이 0명 또는 2명 이상의 임신을 한 여성에 비해 암 발병률이 낮습니다. 그러나 결론을 도출할 때는 주의해야 합니다. 이것은 단지 상관관계일 뿐 인과관계가 아닐 수도 있습니다!</i>
</p>


# 장점 

부분 의존도 그림의 계산은 **직관적**입니다.
특정 특성 값의 부분 의존성 함수는 모든 데이터 포인트가 해당 특성 값을 가정하도록 하는 경우 평균 예측을 나타냅니다.
제 경험에 따르면, 보통 사람들은 PDP의 아이디어를 빨리 이해합니다.

PDP를 계산한 특성이 다른 특성와 상관없는 경우 PDP는 해당 특성이 평균적으로 예측에 영향을 미치는 방식을 완벽하게 나타냅니다.
상관 관계가 없는 경우 **해석이 명확합니다**:
부분 의존도 그림은 j번째 특성이 변경될 때 데이터 집합의 평균 예측이 어떻게 변하는지 보여 줍니다.
특성가 상관 관계일 때 더 복잡합니다. 단점도 참조하십시오.

부분 의존도는 **구현하기 쉽습니다**.

부분 의존도 그림에 대한 계산에는 **원인 해석**이 있습니다.
우리는 특성에 개입하고 예측의 변화를 측정합니다.
이를 통해 특성과 예측 사이의 인과관계를 분석합니다.[^2]
이 관계는 모형에 인과관계가 있습니다. 왜냐하면 우리는 그 결과를 특성의 함수로 명시적으로 모델링하기 때문입니다. 하지만 현실 세계에서는 꼭 그렇지는 않습니다!

# 단점

부분 의존성 함수에서 현실적인 **최대 특성 수**는 2개입니다.
이것은 PDP의 잘못이 아니라 2차원 표현(종이 또는 화면)의 잘못이며 또한 3차원 이상을 상상할 수 없는 우리의 무능함입니다.

일부 PD 그림에는 **특성 분포**가 표시되지 않습니다.
데이터가 거의 없는 영역을 지나치게 해석할 수 있으므로 분포를 생략하는 것은 오해를 유발할 수 있습니다.
이 문제는 깔개(X축의 데이터 포인트에 대한 표시) 또는 히스토그램으로 쉽게 해결됩니다.

독립성에 대한 **가정**은 PD 플롯의 가장 큰 문제입니다.
부분 의존성이 계산되는 특성은 다른 특성과 상관관계가 없는 것으로 가정합니다.
예를 들어, 사람의 몸무게와 키를 고려해 볼 때, 사람이 얼마나 빨리 걷는지 예측하고 싶다고 가정해 보세요.
예를 들어 높이와 같은 특성의 부분 의존성에 대해서는 다른 특성(가중치)이 높이와 상관관계가 없다고 가정하는데 이는 명백히 잘못된 가정입니다.
특정 높이(예: 200 cm)에서 PDP를 계산할 때, 우리는 무게의 한계 분포를 평균합니다. 이 분산은 50 kg 미만일 수 있습니다. 이는 2m인 사람에게 비현실적입니다.
다시 말해 다음과 같습니다.
특성이 상관관계가 있으면 실제 매우 낮은 확률로 특성 분포 영역에 새 데이터 지점을 만듭니다(예: 키가 2m이지만 무게가 50kg 미만일 가능성은 낮음).
이 문제에 대한 한 가지 해결책은 한계 분포 대신 조건부로 작동하는 [Acculated Local Effect Plots](https://tootouch.github.io/IML/accumulated_local_effects/) 또는 짧은 ALE 그림입니다.

**PD 그림에는 평균 한계 효과만 표시되므로 이질적인 효과가 숨겨질 수 있습니다**.
특성의 경우 데이터의 절반이 예측과 양의 연관성을 가지고 있다고 가정합니다. 특성 값이 클수록 예측 값이 크고, 나머지 절반은 음의 연관성을 가집니다. 특성 값이 작을수록 예측이 크다고 가정합니다.
데이터 집합의 두 반쪽 데이터의 효과로 인해 서로 상쇄될 수 있으므로 PD 곡선은 수평선이 될 수 있습니다.
그런 다음 특성이 예측에 영향을 미치지 않는다고 결론을 내립니다.
집계된 선 대신 [개별 조건부 기대 곡선](https://tootouch.github.io/IML/individual_conditional_expectation/)을 표시함으로써 이질적인 효과를 확인할 수 있습니다.

# 소프트웨어 및 대안책

PDP를 구현하는 여러 R 패키지가 있습니다.
예를 들어 iml 패키지를 사용했지만 `pdp`나 `DALEX`도 있습니다.
Python에서는 부분 의존도 플롯이 `scikit-learn`으로 기본 설정되며 `PDPBox`를 사용할 수 있습니다.

이 책에 제시된 PDP의 대안은 [ALE 그림](https://tootouch.github.io/IML/accumulated_local_effects/)과 [ICE 곡선](https://tootouch.github.io/IML/individual_conditional_expectation/)입니다.

---

[^1]: Friedman, Jerome H. "Greedy function approximation: A gradient boosting machine." Annals of statistics (2001): 1189-1232.

[^2]: Zhao, Qingyuan, and Trevor Hastie. "Causal interpretations of black-box models." Journal of Business & Economic Statistics, to appear. (2017).