---
title:  "2.1 Importance of Interpretability"
permalink: /IML/importance_of_interpretability/
---

머신러닝 모델이 잘 되는데도 왜 우리는 모델을 바로 신뢰하지 못하고, 왜 그런 결정이 내려졌는지 무시할 수 없을까? "문제는 바로 분류 문제의 정확도(accuracy)와 같은 단일 평가지표 때문이다. 이는 대부분 현실문제에서 사용하기에는 불완전한 지표이다." (Doshi-Velez and Kim 2017)

왜 해석가능성이 중요한지 더 깊게 한 번 알아도록 합시다. 예측 모델에 대해서 다음과 같이 두 가지 중 고려해야합니다. 하나는 단지 무엇을 예측했는지만 필요한가 입니다. 예를 들자면 고객이 이탈할 확률이나 약들이 환자에게 얼마나 영향이 있는지입니다. 다른 하나는 모델의 성능이 조금 떨어진다 하더라도 예측값이 왜 그렇게 나왔는지 알기를 원하는가 입니다. 어떤 경우에는 의사결정의 원인을 상관하지않고 단지 평가 데이터에 모델의 예측결과가 좋았는지만 알아도 충분합니다. 그러나 다른 경우에는 "이유"를 아는 것이 문제, 데이터 그리고 모델이 실패한 원인에 대해 더 잘 알 수 있도록 도와줍니다. 어떤 모델은 위험도가 낮은 환경, 즉 실수해도 심각한 결과를 초래하지 않거나 (예: 영화 추천 시스템) 해당 방법이 이미 널리 연구되어져 있거나 평가되어진 경우 (예: OCR) 크게 설명력에 대해 필요로하지 않을 수도 있습니다. 해석가능성에 대한 필요는 문제에 대한 불완전성에서 비롯됩니다 (Doshi-Velez and Kim 2017). 즉, 특정 문제나 작업에 대해 예측값(the **what**)만 얻는것으로는 충분하지 않은 상황을 말합니다. 정확한 예측만이 여러분이 가진 문제를 부분적으로나마 해결할 수 있기 때문에 모델은 반드시 예측값이 어떻게 나오게됐는지 설명해야합니다(the **why**).  

**인간의 호기심과 학습**: 사람은 예기치 못한 일이 발생했을 때 환경을 업데이트 하는 정신적 모델을 가지고 있습니다. 업데이트는 예상치 못한 일에 대한 설명력을 찾아야 발생됩니다. 예를 들자면, 한 사람이 갑자기 아픈 기운이 느껴져서 "내가 왜 아픈거 같지?"라고 물었습니다. 그는 항상 딸기를 먹으면 아프다는 것을 알게됐습니다. 그는 정신적 모델을 업데이트하고 딸기는 몸을 아프게하기 때문에 피해야한다고 결심했습니다. 연구에서 불투명한 모델이 사용된다면 모델은 설명력없이 예측값만 낼 수 있기 때문에 과학적 발견은 완전히 감춰진채 남게될 것입니다. 특정 결과값에 대한 이유나 기계에 의해 생긴 행동들 같은 호기심을 만족시키고 학습을 촉진시키기 위해서는 해석가능성과 설명성은 필수입니다. 물론 사람은 모든 순간에 설명을 필요로 하진 않는다. 대부분 사람들은 컴퓨터가 어떻게 작동하는지 알지 못해도 상관없을 것입니다. 예기치 못한 일이 우리에게 호기심을 불러일으킵니다. 예를 들어 왜 컴퓨터가 갑자기 셧다운이 된거지?와 같은 상황이 있습니다.

학습이란 세상 속에서 무언가를 발견하려는 사람의 바람과 관련이 있습니다. 우리는 지적 구조의 요소들간의 모순이나 불일치를 해결하고자 합니다. "이전에 한번도 그런적이 없었는데 왜 갑자기 내 강아지가 나를 물었을까요?" 라고 어떤이가 물었습니다. 여기에는 개의 이전 행동과 새롭게 한 행동간의 모순이 있습니다. 수의사는 이러한 강아지 주인의 모순에 대해 설명해줍니다. "이 개는 스트레스를 받고 있었고 그래서 물었습니다." 더 많은 기계의 의사결정이 사람들의 삶에 영향을 줄수록 그 기계의 행동들에 대한 설명을 하는 것이 더 중요해집니다. 만약 머신러닝 모델이 대출 신청을 기각한다면 대출자의 입장에서는 완전 당황스러울 것 입니다. 이 사람들은 몇 가지 설명만으로 예측했던 상황과 현실의 불일치를 해결할 수 있습니다. 설명성이란 해당 상황에 대해 모든걸 다 설명해야 한다는 것은 아니지만 주 원인에 대해서는 얘기해야합니다. 또다른 예시로는 상품 추천에 대한 알고리즘입니다. 개인적으로 저는 특정 상품이나 영화가 어떤 알고리즘으로 저에게 추천되는지에 대한 원인을 항상 생각합니다. 종종 정확하기도 합니다. 인터넷에서 광고가 저를 따라다니는데 최근에 제가 세탁기를 샀기 때문에 다음날 세탁기 관련 광고가 따라다닐거란 것을 알고있습니다. 장바구니에 겨울 모자가 있다면 장갑을 저에게 권할 수도 있습니다. 영화 추천에서는 제가 좋아하는 영화를 본 다른 유저들이 제가 보지 못한 영화가 있다면 저에게 추천해줄 수 있습니다. 점점 더 많은 온라인 회사들이 추천에 대해 설명설을 추가해나가고 있습니다. 좋은 예시는 주로 구매한 상품들에 대한 조합 기반으로 하는 아마존 상품 추천입니다. 

<p align='center'>
    <img src="https://christophm.github.io/interpretable-ml-book/images/amazon-freq-bought-together.png"><br>
    <i>그림 2.1: 아마존에서 물감을 사게되면 추천되는 상품들</i>
</p>

많은 과학 분야에서는 질적 방법에서 양적 방법(예: 사회학, 심리학)으로, 그리고 머신러닝(생물학, 유전체학)으로의 변화가 있습니다. 과확의 목표라고 한다면 지식을 얻는 것입니다. 그러나 많은 문제들이 빅데이터와 블랙박스 모델들로 해결됩니다. 모델 그자체가 데이터 대신 지적 자산이 됩니다. 해석가능성은 모델에 의해 기록된 지식들을 얻게 할 수 있습니다.

기계학습 모델은 평가와 적절한 지표가 필요한 현실의 문제들을 다룹니다. 자율 주행 자동차가 딥러닝 모델로 자전거타는 사람들을 검출한다고 생각해 봅시다. 자전거타는 사람을 대상으로 평가하는 것을 좋지못한 결과를 초래할 수 있기때문에 훈련 데이터에 에러가 없는지 100% 확인해야합니다. 설명성은 가장 중요한 특징이 자전거의 두 바퀴를 인식하고 있는지 확인할 수 있고 이 설명성을 통해 부분적으로 바퀴를 덮는 사이드에 달린 가방에 대해서 어떻게 해결할지 고민할 수 있도록 도와줍니다. 

기본적으로 머신러닝 모델은 학습 데이터로부터 편향을 학습하게 됩니다. 이 편향이 여러분의 머신러닝 모델을 보호집단에 대해 차별하는 인종차별주의자로 만들 수 있습니다. 해석가능성은 머신러닝 모델에서 편향을 찾을 수 있는 디버깅 도구로도 유용하게 사용될 수 있습니다. 신용대출 신청에 대해 승인 또는 거절하는 자동화를 위해 학습된 모델이 소수를 차별하게 될 수도 있습니다. 여러분의 주 목표는 다시 갚을 수 있는 사람에게만 대출을 승인해주는 것입니다. 이 경우 문제 정의의 불완전성은 대출 채무 불이행을 최소로 할 뿐만 아니라 특정 인구 통계를 근거로 차별하지 않아야 한다는 사실에 있습니다. 이는 머신러닝 모델이 최적화하는 손실 함수로 해결할 수 없는 문제(위험이 낮고 준수하는 방식으로 대출)의 일부인 추가적인 제약 조건입니다.

기계와 알고리즘을 일상생활로 통합하는 과정은 **사회적 수용(social acceptance)**을 증가시키기 위해 해석가능성이 필요합니다. 사람들은 믿음, 욕망, 의도 등을 물체 탓으로 돌립니다. 유명한 실험에서, Heider and Simmel(1944년)[^2]은 참가자들에게 원이 "문"을 열어 "방"으로 들어가는 모양 비디오를 보여주었습니다. 참가자들은 모양에 의도, 감정, 성격적 특성을 부여하면서 인간 에이전트의 행동을 묘사할 때 형상의 행동을 묘사했습니다. 로봇은 제가 "Doge"라고 이름 붙인 제 진공 청소기로 좋은 예시를 들 수 있습니다. 만약 Doge가 막히면, 저는 이렇게 생각합니다: "Doge는 계속 청소를 하고 싶지만, 막혔기 때문에 제게 도움을 요청하는구나." 나중에 Doge가 청소를 마치고 재충전을 위해 홈 베이스를 검색할 때, 저는 이렇게 생각합니다: "Doge는 재충전 욕구가 있고 홈 베이스를 찾을 작정이구나." 저는 또한 성격적인 특성들을 가지고 있습니다: "Doge는 약간 멍청하지만, 귀여운 면이 있네." 이것들은 제 생각입니다. 특히 제가 Doge가 정성스럽게 청소를 하면서 식물을 넘어뜨렸다는 것을 알게 되었을 때 말이죠. 예측을 설명하는 시스템 또는 알고리즘은 더 많은 수용을 찾을 수 있습니다. 설명은 사회적 과정이라고 주장하는 [설명에 관한  챕터](https://tootouch.github.io/IML/human_friendly_explanations/)를 참조하세요.

설명은 사회적 상호작용을 관리할때 사용된다. 어떤 공동의 의미(a shared meaning of something)를 만들어냄으로써, 설명자는 설명 받는 사람의 행동, 감정, 그리고 믿음에 영향을 미칩니다. 기계가 우리와 상호작용을 하기 위해서는 우리의 감정과 믿음을 형성해야 할지도 모릅니다. 기계는 의도한 목표를 달성할 수 있도록 우리를 "설득"시켜야 합니다. 로봇 진공청소기가 작동에 대해 어느 정도 설명하지 않는다면 저는 로봇 청소기를 완전히 이해할 수 없을 것입니다. 예를 들어 진공 청소기는 "사고"(또 화장실 카펫에 걸리는 것과 같은)라는 공유의 의미를 만들어냅니다. 단순히 아무 말도 없이 일하는 것을 멈추는 대신 "사고"가 생겼다고 설명합니다. 흥미롭게도 설명하는 기계의 목표(신뢰를 만드는 것)와 받아들이는 사람의 목표(예측이나 행동을 이해) 사이에 오해가 있을 수 있습니다. 아마도 Doge가 멈춘 이유에 대한 모든 설명은 배터리가 매우 부족하고, 바퀴 하나가 제대로 작동하지 않으며, 장애물이 있음에도 불구하고 로봇을 같은 장소로 계속 가게 하는 버그가 있다는 것일 수 있습니다. 이러한 이유들로 인해 로봇이 꼼짝 못하게 되었지만, 그것은 단지 무언가가 방해가 되었다고 설명했을 뿐이고, 그것은 제가 로봇의 행동을 믿고 그 사고에 대한 공동의 의미를 얻기에 충분했습니다. 그런데 Doge는 또 화장실에 갇혔습니다. Doge가 진공 청소하기 전에는 매번 카펫을 제거해야 해야합니다.

<p align='center'>
    <img src="https://christophm.github.io/interpretable-ml-book/images/doge-stuck.jpg"><br>
    <i>그림 2.2: 진공청소기 Doge가 멈췄다. Doge는 사고에 대해 평평한 면으로 가야한다고 설명했다.</i>
</p>

머신러닝 모델은 해석할 수 있을 때만 디버깅하고 검사할 수 있습니다. 영화 추천과 같은 위험성이 낮은 환경에서도, 해석 능력은 개발 후뿐만 아니라 연구 단계에서도 중요합니다. 나중에 모델이 제품에 사용되면 일이 잘못될 수 있습니다. 잘못된 예측에 대한 해석은 오류의 원인을 이해하는 데 도움이 됩니다. 시스템 어떻게 수정할지 방법을 알려줍니다. 어떤 허스키 사진을 늑대로 잘못 분류하는 허스키 대 늑대 분류기의 예를 생각해 봅시다. 해석가능한 머신러닝 방법을 사용하면 이미지에 눈이 쌓여 잘못 분류된 것을 알 수 있습니다. 이 분류기는 눈을 "늑대"로 분류하기 위한 특성으로 사용하는 법을 배웠습니다. 이는 훈련 데이터 에서 늑대와 허스키를 구분하는 측면에서 이해가 될 수 있지만 실제로는 그렇게 구분하지 않습니다.

머신러닝 모델이 의사결정을 설명할 수 있다고 보장할 수 있다면 다음 특성을 쉽게 확인해볼 수 있습니다(Doshi-Velez and Kim 2017).

- 공정성: 예측이 편견 없이 그리고 보호 집단을 암묵적으로 또는 명시적으로 차별하지 않도록 보장합니다.
  해석가능한 모델은 어떤 사람이 대출을 받아서는 안 된다고 결정했는지 이유를 말해줄 수 있고, 그 결정이 학습된 인구통계학적(예: 인종적) 편견에 근거한 것인지 판단하는 것이 더 쉬워집니다.
- 개인 정보: 데이터의 중요한 정보를 보호합니다.
- 신뢰성 또는 견고성: 입력값의 작은 변화가 예측에 큰 변화를 가져오지 않도록 합니다.
- 인과성: 인과 관계만 선택되었는지 확인합니다.
- 믿음: 인간은 블랙박스에 비해 자신의 결정을 설명하는 시스템을 신뢰하는 것이 더 쉽습니다.


**해석성이 필요하지 않은 경우**

다음 시나리오는 우리가 머신러닝 모델을 해석할 필요가 없거나 심지어 원하지 않을 때를 보여줍니다.

모델이 큰 영향을 미치지 않는 경우에는 해석이 필요하지 않습니다. Mike라는 사람이 Facebook 데이터를 바탕으로 그의 친구들이 다음 휴일에 어디로 갈지 예측하는 머신러닝 사이드 프로젝트를 하고 있다고 상상해 봅시다. Mike는 그들이 휴일에 어디로 갈 것인지에 대한 추측으로 그의 친구들을 놀래키는 것을 바랍니다. 모델이 잘못된 경우(최악하게도 Mike에게는 약간의 당혹감만 있을 뿐), Mike가 모델의 출력을 설명하지 못하는 경우에는 문제가 없습니다. 이 경우에는 해석이 불가능해도 상관없습니다. 그러나 마이크가 이러한 휴가 목적지 예측을 바탕으로 사업을 시작한다면 상황은 달라질 것입니다. 만약 모델이 틀리면, 사업이 손해를 볼 수도 있고, 혹은 그 모델이 일부 사람들에게 더 나쁜 영향을 미칠 수도 있습니다. 왜냐하면 학습된 인종적 편견 때문입니다. 이 모델이 경제적이든 사회적이든 간에 상당한 영향을 미치는 즉시 해석성이 관련되게 됩니다. 

문제가 잘 연구되어 졌을때는 해석할 필요가 없습니다. 일부 애플리케이션은 충분히 잘 연구되어 모델에 대한 실제 경험이 충분하며, 모델에 대한 문제는 시간이 지남에 따라 해결되었습니다. 봉투에서 이미지를 처리하고 주소를 추출하는 광학 문자 인식(OCR)을 위한 머신러닝 모델이 좋은 예입니다. 이러한 시스템에는 수년간의 경험이 있으며 잘 작동하고 있습니다. 게다가 이후 맞닥드릴 작업에 대한 추가적인 인사이트를 얻는 것에는 관심이 없습니다.

해석가능성을 통해 사용자 또는 프로그램이 시스템을 조작할 수 있습니다. 시스템을 속이는 사용자 문제는 생성자와 모델 사용자 사이의 불일치로 인해 발생합니다. 신용등급제는 은행이 반환 가능성이 있는 지원자에게만 대출을 해주기를 원하고, 신청자들은 은행이 대출해 주지 않으려 해도 대출을 받기를 목표로 하기 때문에 있는 제도이다. 목표 사이의 이러한 불일치는 지원자들이 대출 기회를 늘리기 위해 시스템을 이용하도록 인센티브를 제공합니다. 신용카드를 두 장 이상 소지한 것이 자신의 점수에 부정적인 영향을 미친다는 것을 신청자가 알고 있다면, 단순히 자신의 점수를 높이기 위해 세 번째 신용카드를 반납하고, 대출 승인이 난 후 새 카드를 정리하면 됩니다. 그러나 신용 점수는 향상되었지만, 실제 대출금 상환 확률은 변하지 않았습니다. 입력 내용이 원인 특성의 대리인 경우에만 시스템을 게임화할 수 있지만 실제로 결과를 초래하지는 않습니다. 가능하다면 대리 특성(proxy features)은 모델을 게임화할 수 있도록 하기 때문에 피해야 합니다. 예를 들어, 구글은 독감 발생을 예측하기 위해 구글 플루 트렌드라는 시스템을 개발했습니다. 시스템이 Google 검색과 독감 발생 간의 상관관계를 확인했는데 성능이 좋지 않았습니다. 검색 쿼리의 분포가 변경되었고 Google Flu Trends가 많은 독감 발생을 놓쳤습니다. Google 검색은 독감을 유발하지 않습니다. 사람들이 "열병"과 같은 증상을 검색할 때 열별은 실제 독감 발병과 관련이 있을 뿐입니다. 이상적으로는 모델이 게임할 수 없기 때문에 인과관계 기능만 사용하는 것이 좋습니다.

---

[^1]: Doshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. http://arxiv.org/abs/1702.08608 (2017).

[^2]: Heider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).