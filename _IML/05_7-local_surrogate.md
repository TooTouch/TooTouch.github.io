---
title:  "5.7 Local Surrogate (LIME)"
permalink: /IML/local_surrogate/
toc: true
---

# Local Surrogate (LIME)

로컬 대리 모델은 블랙박스 머신러닝 모델의 개별 예측을 설명하는 데 사용되는 해석 가능한 모델입니다.
지역적으로 해석 가능한 model-agnostic 설명(LIME)[^1]은 지역 대리 모델을 제안하는 논문입니다.
대리 모델은 기본 블랙박스 모델의 예측에 근사하도록 학습됩니다.
LIME은 글로벌 대리 모델을 학습하는 대신 개별 예측을 설명하기 위해 로컬 대리 모델을 학습하는 데 초점을 맞춥니다.

이는 꽤 직관적으로 알 수 있습니다.
먼저 학습 데이터는 잊어버리고 데이터 포인트를 입력하고 모델의 예측을 얻을 수 있는 블랙박스 모델만 있다고 가정합니다.
원하는 만큼 자주 상자를 확인할 수 있습니다.
여러분의 목표는 머신러닝 모델이 왜 어떤 예측을 했는지 이해하는 것입니다.
LIME은 머신러닝 모델에 변형된 데이터를 제공할 때 예측에 어떤 일이 발생하는지 테스트합니다.
LIME은 변형된 샘플들과 이에 대한 블랙박스 모델의 예측으로 구성된 새 데이터 세트를 생성합니다.
이 새로운 데이터 집합에서 LIME은 해석 가능한 모델을 학습합니다. 이 모델은 샘플링된 관측치와 관심 관측치의 근접성에 의해 가중됩니다.
해석 가능한 모델은 [Lasso](https://tootouch.github.io/IML/linear_regression/) 또는 [decision tree](https://tootouch.github.io/IML/decision_tree/)와 같은 [해석 가능한 모델 장](https://tootouch.github.io/IML/interpretable_models/)의 어떤 것이든 적용할 수 있습니다.
학습된 모델은 머신러닝 모델 예측의 근사치여야 하지만, 전체적으로 근사치일 필요는 없습니다.
이런 종류의 정확성은 지역 충실도라고도 불립니다.

수학적으로 해석하는데 제약 조건이 있는 로컬 대리 모델은 다음과 같이 표현할 수 있습니다.

$$\text{explanation}(x)=\arg\min_{g\in{}G}L(f,g,\pi_x)+\Omega(g)$$

예를 들어 $$x$$에 대한 설명 모델은 손실 $$L$$(예: 평균 제곱 오차)을 최소화하는 모델 $$g$$(예: 선형 회귀 모델)로서, 모델 복잡도 $$\Omega(g)$$는 낮게 유지되는 반면(예: 더 적은 특성) 원래 모델 $$f$$의 예측에 얼마나 가까운지 측정합니다. $$G$$는 가능한 설명 모델의 집합입니다. 예를 들어 해석 가능한 모든 선형 회귀 모델이 있습니다.
근접도 측정값 $$\pi_x$$은(는) 설명에 고려하는 관측치 $$x$$ 주변의 이웃의 크기를 정의합니다.
실제로 LIME은 손실 부분만 최적화합니다.
사용자는 선형 회귀 모델이 사용할 수 있는 최대 특성 수를 선택하여 복잡성을 결정해야 합니다.

지역 대리 모델을 학습하는 방법은 다음과 같습니다.

- 블랙박스 예측에 대한 설명을 원하는 관심 관측치를 선택합니다.
- 데이터 세트를 변형하고 이러한 새로운 포인트에 대한 블랙박스 예측을 얻을 수 있습니다.
- 관심 관측치에 근접하는 정도에 따라 새 샘플에 가중치를 계산합니다.
- 데이터 세트의 가중치 적용 및 해석 가능한 모델을 변형된 데이터로 학습합니다.
- 로컬 모델을 해석하여 예측을 설명합니다.

[R](https://github.com/thomasp85/lime) 및 [Python](https://github.com/marcotcr/lime)에서는 선형 회귀 분석을 해석 가능한 대리 모델로 선택할 수 있습니다.
해석 가능한 모델에서 원하는 특성 수인 K를 미리 선택해야 합니다.
K가 낮을수록 모델을 쉽게 해석할 수 있습니다.
K가 높으면 잠재적으로 충실도가 높은 모델을 생산할 수 있습니다.
K 특성을 모두 갖춘 모델을 학습하는 방법에는 여러 가지가 있습니다.
[Lasso](https://tootouch.github.io/IML/linear_regression/)가 좋은 선택입니다.
정규화 파라미터 $$\lambda$$가 높은 lasso 모델은 아무런 특성 없는 모델을 생성합니다.
Lasso 모델을 서서히 $$\lambda$$를 줄임으로써 그 특징들은 0과 다른 가중치 추정치를 얻게 됩니다.
모델에 K 특성이 있는 경우 원하는 특성 수를 얻게됩니다.
다른 방법은 전진 또는 후진 선택입니다.
즉, 전체 모델(= 모든 특성 포함)에서 시작하거나 절편만 있는 모델에서 시작한 다음 K 특성이 있는 모델에 도달할 때까지 추가 또는 제거 시 어떤 특성이 가장 크게 개선되는지 테스트할 수 있습니다.

데이터의 변형을 어떻게 얻을까요?
이는 텍스트, 이미지 또는 표 형식의 데이터 유형에 따라 달라집니다.
텍스트 및 이미지의 경우 단일 단어나 슈퍼 픽셀을 켜거나 끄는 것이 해결책입니다.
표 데이터의 경우 LIME은 각 특성을 개별적으로 섞거나 하여 특성에서 가져온 평균 및 표준 편차를 사용하여 정규 분포에서 새 샘플을 작성합니다.

# LIME 표 데이터

표 데이터는 테이블로 제공되는 데이터이며 각 행은 관측치를 나타내고 각 열은 특성를 나타냅니다.
LIME 샘플은 관심의 관측치가 아니라 학습 데이터가 몰린 부분에서 추출되는 문제가 있습니다.
그러나 이는 일부 샘플 포인트 예측의 결과가 관심 데이터 지점과 다를 수 있고 LIME이 최소한 몇 가지 설명을 학습할 수 있는 확률을 높입니다.

샘플링 및 로컬 모델 학습이 어떻게 작동하는지 시각적으로 설명하는 것이 가장 좋습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/lime-fitting-1.png'><br>
    <i>그림 5.33: 표 데이터에 대한 LIME 알고리즘입니다. A) 주어진 랜덤 포리스트 예측 특성은 x1 및 x2입니다. 예측 클래스: 1(어두움) 또는 0(밝음)입니다. B) 관심의 관측치(큰 점)와 정규 분포(작은 점)에서 샘플링된 데이터이다. C) 관심 관측치 근처의 포인트에 더 큰 가중치를 부여합니다. D) 격자 표시는 가중치 샘플에서 로컬로 학습한 모델의 분류를 나타냅니다. 흰색 선은 의사결정 경계를 표시합니다(P(class=1) = 0.5).</i>
</p>

항상 그렇듯이, 문제는 세부적인 것에 있습니다.
의미 있는 이웃을 정의하는 것은 어렵습니다.
LIME은 현재 지수 평활 커널을 사용하여 주변을 정의합니다.
스무딩 커널은 두 개의 데이터 관측치를 사용하여 근접 측정값을 반환하는 함수입니다.
커널 너비는 이웃의 크기를 결정합니다.
커널 폭이 작다는 것은 관측치가 로컬 모델에 영향을 미치려면 매우 가까워야 함을 의미하며, 커널 폭이 크면 멀리 있는 관측치도 모델에 영향을 미칩니다.
[LIME의 Python 구현(파일 라임/lime_tabular.py)](https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606)을 보면 (정상화된 데이터에 대해) 지수 평활 커널을 사용하고 커널 너비는 학습 데이터의 열 개수 제곱근의 0.75를 곱합니다.
가장 큰 문제는 우리가 최적의 커널이나 폭을 찾을 수 있는 좋은 방법이 없다는 것입니다.
0.75는 어디서 나오는 걸까요?
특정 시나리오에서는 다음 그림과 같이 커널 너비를 변경하여 설명을 쉽게 해볼 수 있습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/lime-fail-1.png'><br>
    <i>그림 5.34: 관측치 x = 1.6의 예측에 대한 설명입니다. 단일 특성에 따른 블랙박스 모델의 예측은 두꺼운 선으로 표시되고 데이터의 분포는 x축 위에 선들과 함께 표시됩니다. 서로 다른 커널 폭을 가진 세 가지 로컬 대리 모델을 계산합니다. 그 결과 선형 회귀 모델은 커널 너비에 따라 달라집니다. 특성이 x = 1.6에 대해 긍정, 부정 또는 아무 효과도 없나요?</i>
</p>

이 예에서는 하나의 특성만 보여 줍니다.
고차원 특성 공간에서는 상황이 더 악화됩니다.
또한 거리 측정이 모든 특성을 동등하게 취급해야 하는지도 매우 불분명합니다.
x1의 거리 단위가 x2의 단위와 동일할까요?
거리 측정은 상당히 임의적이며 서로 다른 치수(각 특성)의 거리는 전혀 비교할 수 없을 수 있습니다.


## 예시

구체적인 예를 살펴보겠습니다.
[자전거 대여 데이터](https://tootouch.github.io/IML/bike_rentals/)로 돌아가서 예측 문제를 분류로 합니다.
시간이 지나면서 자전거 렌탈이 더욱 인기를 끌게 된 추세를 감안한 후, 특정 날짜에 대여된 자전거의 수가 평균 이상인지 이하인지를 알고자 합니다.
또한, "이상"은 평균 자전거 수보다 높고, 추세에 맞게 조정된 것으로 해석할 수 있습니다.


먼저 분류 문제에 100개 트리가 있는 랜덤 포레스트를 훈련합니다.
날씨와 달력 정보를 기준으로 대여 자전거의 수가 평균 이상일 때는 언제입니까?

설명은 2개의 특성으로 만들었습니다.
예측 클래스가 서로 다른 두 관측치에 대해 학습한 희소 로컬 선형 모델의 결과는 다음과 같습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/lime-tabular-example-explain-plot-1-1.png'><br>
    <i>그림 5.35: 자전거 대여 데이터 세트의 두 가지 관측치에 대한 LIME 설명입니다. 따뜻한 온도와 좋은 날씨 상황은 예측에 긍정적인 영향을 미칩니다. x축은 다음과 같은 특성 효과를 나타냅니다. 가중치는 실제 특성 값을 곱한 값입니다.</i>
</p>

그림에서는 숫자 특성보다 범주형 특성를 해석하는 것이 더 쉽다는 것을 알 수 있습니다.
한 가지 해결책은 숫자형을 범주형으로 분류하는 것입니다.


# LIME 텍스트 데이터

텍스트의 LIME은 표 데이터의 LIME과 다릅니다.
데이터의 변형은 다음과 같이 다르게 생성됩니다.
원본 텍스트에서 시작하여 원본 텍스트에서 임의로 단어를 제거하여 새 텍스트를 만듭니다.
데이터 세트는 각 단어에 대한 이진 특성으로 표시됩니다.
특성은 해당 단어가 포함된 경우 1이고 제거된 경우 0입니다.

## 예시

이 예에서는 [YouTube 댓글](https://tootouch.github.io/IML/youtube_spam_comments/)을 스팸 또는 일반으로 분류합니다.

블랙박스 모델은 문서 단어 행렬에서 학습받은 심층 의사결정 트리입니다.
각 설명은 하나의 문서(= 하나의 행)이고 각 열은 지정된 단어의 발생 횟수입니다.
짧은 결정트리는 이해하기 쉽지만, 이 경우 트리는 매우 깊습니다.
또한 이 트리 대신 순환 신경망 또는 단어 임베딩(추상 벡터)에 대해 훈련된 서포트 벡터 머신을 사용할 수 있습니다.
이 데이터셋과 해당 클래스의 두 가지 댓글를 살펴보겠습니다(스팸의 경우 1, 일반 댓글의 경우 0).

| INDEX | CONTENT                                 | CLASS |
| ----- | --------------------------------------- | ----- |
| 267   | PSY is a good guy                       | 0     |
| 173   | For Christmas Song visit my channel! ;) | 1     |


다음 단계는 로컬 모델에서 사용되는 데이터 세트의 일부 변형을 생성하는 것입니다.
예를 들어, 댓글의 몇 가지 변형이 있습니다.

| INDEX | For | Christmas | Song | visit | my  | channel! | ;)  | prob | weight |
| ----- | --- | --------- | ---- | ----- | --- | -------- | --- | ---- | ------ |
| 2     | 1   | 0         | 1    | 1     | 0   | 0        | 1   | 0.17 | 0.57   |
| 3     | 0   | 1         | 1    | 1     | 1   | 0        | 1   | 0.17 | 0.71   |
| 4     | 1   | 0         | 0    | 1     | 1   | 1        | 1   | 0.99 | 0.71   |
| 5     | 1   | 0         | 1    | 1     | 1   | 1        | 1   | 0.99 | 0.86   |
| 6     | 0   | 1         | 1    | 1     | 0   | 0        | 1   | 0.17 | 0.57   |

각 열은 문장의 한 단어에 해당합니다.
각 행은 변화된 샘플이며, 1은 단어가 이 변화의 일부라는 것이고 0은 단어가 제거되었음을 의미합니다.
변화된 샘플 중 하나에 해당하는 문장은 "`Christmas Song visit my ;)`"입니다.
"prob" 열은 각 변형된 문장에 대한 스팸의 예측 가능성을 나타냅니다.
"weight" 열은 제거된 단어의 비율을 1 빼기 단어의 비율로 계산되어 원래 문장에 대한 변화의 근접도를 나타냅니다. 예를 들어, 7개 단어 중 1개를 제거한 경우 근접도는 1 - 1/7 = 0.86입니다.


다음은 LIME 알고리즘에 의해 발견된 추정 로컬 가중치와 함께 두 개의 문장(스팸 1개, 스팸 없음 1개)입니다.

| case | label_prob | feature  | feature_weight |
| ---- | ---------- | -------- | -------------- |
| 1    | 0.1701170  | good     | 0.000000       |
| 1    | 0.1701170  | a        | 0.000000       |
| 1    | 0.1701170  | is       | 0.000000       |
| 2    | 0.9939024  | channel! | 6.180747       |
| 2    | 0.9939024  | For      | 0.000000       |
| 2    | 0.9939024  | ;)       | 0.000000       |

"channel!"이라는 단어는 스팸의 높은 가능성을 나타냅니다.
스팸이 아닌 데이터의 설명의 경우 어떤 단어가 제거되더라도 예측 클래스는 동일하게 유지되므로 0이 아닌 가중치를 추정하지 않았습니다.

# LIME 이미지 데이터

*이 섹션은 Verena Haunschmid가 작성했습니다.*

영상의 LIME은 표 데이터 및 텍스트의 LIME과 다르게 작동합니다.
직관적으로, 둘 이상의 픽셀이 한 클래스에 기여하기 때문에 개별 픽셀을 동요시키는 것은 별로 의미가 없습니다.
개별 픽셀을 임의로 변경해도 예측이 크게 변경되지는 않을 수 있습니다.
따라서 영상을 "슈퍼픽셀"로 분할하고 슈퍼픽셀을 끄거나 켜서 영상의 변형이 만들어집니다.
슈퍼픽셀은 비슷한 색상의 상호 연결된 픽셀이며 각 픽셀을 회색과 같은 사용자 정의 색상으로 교체하여 끌 수 있습니다.
사용자는 각 순열에서 슈퍼픽셀을 끌 확률을 지정할 수도 있습니다.

## 예시

이미지 설명 계산이 다소 느리기 때문에, [lime R 패키지](https://github.com/thomasp85/lime)에는 사전에 예시로 계산된 결과가 있습니다.
설명은 영상 샘플에 직접 표시할 수 있습니다.
이미지당 여러 개의 예측 라벨을 가질 수 있기 때문에(확률별로 정렬됨) 상위 `n_labels`를 설명할 수 있습니다.
다음 이미지에서 상위 3개의 예측은 *electric guitar*, *acoustic guitar* 및 *Labrador*였습니다.

<p align='center'>
    <img src='https://christophm.github.io/interpretable-ml-book/images/lime-images-package-example-1.png'><br>
    <i>그림 5.36: Google의 Inception 신경망에 의해 만들어진 이미지 분류 상위 3개 클래스에 대한 LIME 설명입니다. 이 예는 LIME 문서에서 가져온 것입니다(Ribeiro et al., 2016).</i>
</p>

첫 번째 경우의 예측과 설명은 매우 그럴듯합니다.
*electric guitar*의 첫 번째 예측은 물론 틀렸습니다. 하지만 설명에 따르면, 신경망은 여전히 합리적으로 행동했습니다. 왜냐하면 식별된 이미지 부분이 electric guitar일 수 있기 때문입니다.

# 장점

**기존 학습 모델**을 교체하더라도 동일한 로컬 해석 가능한 모델을 사용하여 설명할 수 있습니다.
설명을 보는 사람들이 의사결정 트리를 가장 잘 이해한다고 가정해보세요.
지역 대리 모델을 사용하기 때문에 의사결정 트리를 학습 모델로 사용하지 않고 설명으로 사용할 수 있습니다.
예를 들어 SVM을 사용할 수 있습니다.
또한 xgboost 모델이 더 잘 작동하는 것으로 판명된 경우에는 SVM을 교체하고 의사결정 트리로 예측을 설명할 수 있습니다.

lasso 또는 짧은 트리를 사용할 경우, 그 결과로 나타나는 **설명은 짧고(= 선택적) 대조적**일 수 있습니다.
따라서 [인간 친화적인 설명](https://tootouch.github.io/IML/human_friendly_explanations/)을 합니다.
이것이 제가 LIME을 더 많이 보는 이유입니다. 설명이 필요한 일반인이나 시간이 거의 없는 사람일 때 말이죠.
완전한 설명을 하기에는 충분하지 않기 때문에 LIME이 법적으로 예측을 충분히 설명해야하는 상황에서는 사용되지 않습니다.
또한 머신러닝 모델을 디버깅하는 경우 몇 가지 이유 대신 모든 이유를 갖는 것이 유용합니다.

LIME은 **표 형식의 데이터, 텍스트 및 이미지**에서 작동하는 몇 안되는 방법 중 하나입니다.

**충성도 측정**(해석 가능 모델은 블랙박스 예측에 얼마나 근사하는지)은 데이터 관측치 주변에서 해석 가능한 모델이 블랙박스 예측을 설명하는 데 있어 얼마나 신뢰할 수 있는지 잘 알 수 있습니다.

LIME은 Python([lime](https://github.com/marcotcr/lime) 라이브러리) 및 R([lime package](https://cran.r-project.org/web/packages/lime/index.html)) 및 [iml package](https://cran.r-project.org/web/packages/iml/index.html))에서 구현되어 있으며 **사용이 매우 간편합니다.**

로컬 대리 모델을 **사용하여 작성된 설명은 원래 모델이 학습한 것과 다른 (해석 가능한) 특성을 사용할 수 있습니다.**.
물론 이러한 해석 가능한 특성은 데이터 관측치에서 파생되어야 합니다.
텍스트 분류기는 함축된 단어 임베딩에 의존할 수 있지만, 설명은 문장에 단어가 있는지 없는지를 기준으로 할 수 있습니다.
회귀 모델은 일부 특성의 해석 불가능한 변환에 의존할 수 있지만, 설명은 원래 특성을 사용하여 작성할 수 있습니다.
예를 들어, 회귀 분석 모델은 조사에 대한 답의 PCA(주요 구성 요소 분석)의 구성 요소에 대해 학습될 수 있지만, LIME은 원래 조사 질문에 대해 학습될 수 있습니다.
해석 가능한 특성을 LIME에 사용하는 것은 다른 방법보다 큰 이점이 될 수 있습니다. 특히 모델을 해석할 수 없는 특성으로 학습한 경우에는 더욱 그렇습니다.

# 단점

LIME을 표와 함께 사용할 때 이웃에 대한 올바른 정의는 아주 어렵고 해결되지 않은 문제입니다.
제 생각에 이는 LIME의 가장 큰 문제이며 제가 LIME을 매우 주의 깊게 사용하도록 권하는 이유입니다.
각 응용 프로그램에 대해 서로 다른 커널 설정을 시도하고 설명이 적절한지 직접 확인해야 합니다.
불행히도, 이는 좋은 커널 폭을 찾기 위해 할 수 있는 최선의 충고입니다.

LIME의 현재 구현에서는 샘플링을 더 개선해야합니다.
데이터 점은 특성 간의 상관 관계를 무시하고 가우스 분포에서 샘플링됩니다.
이로 인해 데이터 지점이 발생할 수 있으며, 이를 통해 로컬 설명 모델을 학습할 수 있습니다.

설명 모델의 복잡성을 미리 정의해야 합니다.
이는 작은 불만 사항일 뿐입니다. 결국 사용자는 항상 충실도와 희소성이 절충되는 것을 정의해야 하기 때문입니다.

또 다른 큰 문제는 설명의 불안정성입니다.
논문에서, 저자들은 두 개의 매우 가까운 관측치에 대한 설명이 시뮬레이션 환경에서 크게 다르다는 것을 보여주었습니다[^2].
또, 제 경험에 의하면, 샘플링 과정을 반복한다면, 나오는 결과가 다를 수 있습니다.
불안정성은 설명을 신뢰하기 어렵다는 것을 의미하며, 여러분은 매우 비판적이어야 합니다.

결론은 다음과 같습니다.
LIME을 구체적으로 구현하는 지역 대리 모델은 매우 유망합니다.
그러나 이 방법은 아직 개발 단계에 있으며, 많은 문제들이 해결되어야 안전하게 적용될 수 있습니다.

---

[^1]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should I trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).

[^2]: Alvarez-Melis, David, and Tommi S. Jaakkola. "On the robustness of interpretability methods." arXiv preprint arXiv:1806.08049 (2018).