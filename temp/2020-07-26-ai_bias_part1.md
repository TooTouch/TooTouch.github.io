---
title: "Bias in Data-driven AI Systems -
An Introductory Survey - Part1: Understanding Bias"
categories: 
    - Paper Review
toc: true
---

Bias는 머신러닝에서 오래전부터 있던 개념입니다. 이번 파트는 AI 시스템이 내린 의사 결정에서의 경향(inclination) 또는 편견(prejudice)에 대한 내용입니다. 구제적으로는 어떻게 bias가 AI 시스템에 발생하는지와 데이터에 어떻게 나타나는지에 대해 얘기하고 공정함(fairness)의 정의는 과연 무엇인지와 마지막으로 데이터 수집 과정과 bias에 대한 정의에 대해 규제가 필요한지를 다룹니다.

# Socio-technical causes of bias

AI는 데이터에 많이 의존합니다. 그리고 그 데티어는 사람에 의해 직접적으로 수집되기도하고 또는 사람이 만든 시스템에 의해 간접적으로 수집되기도 합니다. 즉, AI는 결국 사람의 영향을 받기 때문에 사람에 의한 bias가 포함되게 됩니다. 그리고 이렇게 발생된 bias는 복장한 사회구조에 따라 그 차이가 과장될 수 있습니다. 우리의 사회속에는 이미 bias가 존재합니다. 사회 속에서 특정 집단은 혜택을 받을 수 있지만 반면에 그만큼 혜택을 받지 못하는 집단이 있기도 합니다. 알고리즘은 이러한 구조를 기반으로 만들어진 것이기 때문에 자연스럽게 사회에 존재하는 bias를 가져오게 됩니다. 문제는 이러한 알고리즘과 그 구조 간의 복잡한 관계가 우리 사회속에 어떤 영향을 줄지 명확하게 알 수 없기 때문에 이런 문제를 해결할 수 있는 "알고리즘의 설명가능성(algorithmic accountability)"가 필요합니다.

# How is bias manifetsed in data

데이터에서 발견할 수 있는 bias는 각 속성들 간의 관계에서 확인할 수도 있고 특정 집단에서 나타날 수도 있습니다

## Sensitive fetures and causal influences

데이터는 사람들의 행동을 수치화해서 입력하게 되는데 bias나 차별적인 요소가 있는 속성은 있을 수도 있고 없을 수고 있습니다. 문제는 만약 있다고해도 문제가 되는 속성을 제거한다고 bias가 없는 모델을 학습할 수 있는 것은 아닙니다. 상관관계가 높은 다른 속성이 문제가 되는 속성을 대신해서 영향을 미칠 수 있기 때문인데 예를 들면 미국의 각 도시들의 주민은 인종이라는 속성과 상관관계가 높습니다. 떄문에 은행 대출이나 같은날 구매나 배달 거절에 영향을 미칠 수 있습니다. 그래서 차라리 인종을 포함하는 것이 보다 공정한 모델을 만드는데 도움이 될 수 있습니다. 또는 목적값과 상관관계가 높을 수도 있는데 예를 들어 사고율을 예측하는 모델을 만든다 했을 때 빨간색 차를 타는 사람들이 그런 경향이 높다고 보였을 때 보험사에서는 빨간차를 타는 사람에게 더 높은 보험료를 설정할 수 있습니다. 변수 간 인과적 영향를 발견하고 이해하는 것이 bias를 다루는 기본적인 방법이기도 합니다.

## Representativeness of data

머신러닝 모델은 데이터가 있어야하고 모델은 그 데이터의 패턴을 학습하게 됩니다. 그러나 이러한 데이터는 특정 집단의 특징을 과장해서 나타낼 수도 있고 그렇지 않을 수도 있습니다. 이러한 경우는 해당 집단에 대해 일반화된 속성을 나타낼 수 없기 때문에 모델이 잘못된 특성을 학습하게 됩니다. 이러한 사실 때문에 모델에 문제가 없어도 데이터에서 이런 bias가 있으면 결과도 잘못된 결과를 얻게 됩니다.

## Data modalities and bias

데이터는 수치, 텍스트, 이미지 등 다양한 양식으로 만들어집니다. 또는 음성-텍스트와 같이 복잡한 양식도 있습니다. Bias를 줄이기 위한 머신 러닝 모델의 대부분은 하나의 양식을 고정하여 학습합니다. 성병이나 인종 그리고 시간에 따른 변화와 같은 언어 데이터에서의 bias를 해결하기 위해서 많은 연구들이 있었습니다. 컴퓨터 비전에서는 MNIST(손 글씨 데이터)와 같이 여러 기준이 되는 데이터들을 수집하여 많은 연구들이 이뤄졌지만 이는 학습을 위한 데이터가 현실 데이터의 대표성을 가지고 있다는 것을 가정합니다. 그럼에도 불구하고 현실에서는 이런 데이터가 오히려 bias가 될 수 있습니다. 예를 들어 얼굴 인식에서는 얼굴이 밝은 남성이 어두운 여성보다 더 잘 인식이 되기도 하고 이런 bias가 어디서 발생했는지 알고 싶어도 복잡한 구조때문에 이를 알기가 어렵습니다.

# How is fairness defined?

컴퓨터 과학에서 말하는 공정성(fairnessdp)은 스무개가 넘을 많큼 많은 정의가 있고 각각 서로 다른 기준으로 정의되어 있습니다. 현존하는 공정성에 대한 정의를 다음과 같이 5가지로 분류할 수 있습니다. (1) "predicted outcome", (2) "predicted and actual outcome", (3) "predicted probabilities and actual outcome", (4) "similarity based" and (5) "causal reasoning". 요약하자면 (1)~(3)은 예측값 또는 실제값을 기준으로 하고 (4)~(5)은 이와 반대로 특정 속성과의 관계를 기준으로 정의합니다. 
그러나 문제는 이렇게 다양한 정의가 있음에도 불구하고 여전히 서로 다른 기준에서의 장점과 단점에 대해 공정성을 어떻게 정의하는 것이 옳은지에 대한 논란은 여전히 존재합니다.

# Legal issues of bias and fairness in AI

AI 시스템에서 발생하는 다양한 bias와 사회적 영향에 대해 규제가 필요한지에 대한 의문도 계속되고 있습니다. 현재 EU에서의 규제의 경우는 문제가 되는 의사결정이 발생할 때 적용되지만 데이터의 품질에 대해 적용되는 경우는 거의 없습니다. 차별에 대한 문제를 통제하기 위해 여러 조항들이 생겼지만 이러한 조항들은 아주 구체적인 기준들에 한해서만 결정되고 그 근거가 되는 증거를 필요로 하는데 이를 나타내기가 어렵다는게 문제입니다. [Art. 22 GDPR(General Data Protection Regulation)](https://www.privacy-regulation.eu/en/article-22-automated-individual-decision-making-including-profiling-GDPR.htm)에서는 수학적 또는 통계적 절차를 사용해서 차별적 영향을 예방해야한다고 말합니다. 이 규제의 효율성이 어느정도인지는 명확하진 않지만 자동화된 의사결정이 사용에 대한 규제와 같이 몇 가지 가이드 라인을 제공합니다. 